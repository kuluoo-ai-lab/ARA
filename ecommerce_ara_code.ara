// ==================== ARA E-COMMERCE PLATFORM WITH AGENTIC AI ====================

// ==================== SYSTEM IMPORTS AND CONFIGURATION ====================

import format.web
import format.api
import format.database
import architecture.modular
import architecture.microservices
import architecture.event_driven
import development_plan.agile.scrum
import development_plan.ci_cd
import compliance.gdpr
import compliance.pci_dss
import compliance.hipaa
import component.frontend.react
import component.frontend.redux
import component.frontend.material_ui
import component.backend.django
import component.backend.django_rest_framework
import component.database.postgresql
import component.cache.redis
import component.search.elasticsearch
import component.messaging.rabbitmq
import component.payment.stripe
import component.cloud.aws
import component.monitoring.prometheus
import component.logging.elk_stack
import component.ml.tensorflow
import component.ml.scikit_learn

// ==================== SYSTEM ENVIRONMENT AND CONFIGURATION ====================

ENVIRONMENT="production"
REGION="multi_region_deployment"
REGIONS=["us-east-1", "eu-west-1", "ap-southeast-1", "ap-south-1"]
AVAILABILITY_ZONES=["3_per_region"]

TOKEN_LIMIT=100000
MAX_CONCURRENT_USERS=50000
REQUEST_TIMEOUT=30000
CACHE_TTL_DEFAULT=3600

TECH_STACK={
	frontend: [react.js_18, redux_toolkit, material_ui_5, axios, react_router_v6],
	backend: [django_4.2, django_rest_framework_3.14, celery_5.2, gunicorn],
	database: [postgresql_15, redis_7, elasticsearch_8],
	messaging: [rabbitmq_3.12, kafka_optional],
	payment: [stripe_api_v1, paypal_adaptive_optional],
	cloud: [aws_ec2, aws_rds, aws_s3, aws_cloudfront, aws_elb],
	monitoring: [prometheus, grafana, datadog_optional],
	logging: [elasticsearch, logstash, kibana],
	ml_ai: [tensorflow_2.12, scikit_learn_1.2, pandas, numpy, scipy]
}

SCALING={
	auto_scaling_enabled=true,
	min_instances=5,
	max_instances=100,
	target_cpu_utilization=70,
	target_memory_utilization=75,
	scale_up_threshold=80,
	scale_down_threshold=30,
	cooldown_period=300
}

PERFORMANCE_TARGETS={
	response_time_p50=100,
	response_time_p95=200,
	response_time_p99=500,
	error_rate_target=0.5,
	uptime_target=99.9,
	cache_hit_ratio_target=75,
	database_query_p95=100
}

SECURITY_CONFIG={
	tls_version="1.3",
	cipher_suites=["TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256"],
	certificate_provider="letsencrypt_auto_renewal",
	api_key_rotation_days=90,
	session_timeout_minutes=60,
	password_hash_algorithm="bcrypt_12_rounds",
	mfa_required_for_admin=true,
	ip_whitelist_enabled=true,
	rate_limiting_enabled=true
}

DATA_RETENTION={
	detailed_metrics_days=90,
	aggregated_metrics_days=730,
	logs_detailed_days=30,
	logs_archived_days=365,
	user_activity_logs_days=180,
	payment_logs_days=2555,
	audit_trail_days=2555
}

// ==================== DATA SOURCES AND DATABASE CONFIGURATION ====================

<data>

<data1>
data_name="USER_PROFILE_DATABASE"
data_type="relational"
source="postgresql_primary"
replica="postgresql_replica_multi_az"
visibility="PRIVATE"
encryption="AES_256"
backup_frequency="hourly"
backup_retention="30_days"
compliance=["GDPR", "CCPA"]

SCHEMA={
	users: {
		user_id: uuid_primary_key,
		email: string_unique,
		password_hash: string_bcrypt,
		full_name: string,
		phone_number: string,
		date_of_birth: date,
		gender: enum[M,F,Other],
		profile_image_url: string,
		bio: string,
		account_status: enum[active, suspended, deleted],
		email_verified: boolean,
		phone_verified: boolean,
		mfa_enabled: boolean,
		preferred_language: string,
		timezone: string,
		created_at: timestamp_utc,
		updated_at: timestamp_utc,
		last_login: timestamp_utc,
		account_deleted_at: timestamp_utc_nullable
	},
	user_preferences: {
		user_id: uuid_foreign_key,
		preferred_categories: json_array,
		price_range_min: decimal,
		price_range_max: decimal,
		brand_preferences: json_array,
		notification_preferences: json,
		marketing_email_opted: boolean,
		sms_notifications_enabled: boolean,
		push_notifications_enabled: boolean,
		newsletter_subscribed: boolean,
		theme_preference: enum[light, dark, auto],
		updated_at: timestamp_utc
	},
	user_addresses: {
		address_id: uuid_primary_key,
		user_id: uuid_foreign_key,
		address_type: enum[shipping, billing],
		street_address: string,
		apartment_suite: string_nullable,
		city: string,
		state_province: string,
		postal_code: string,
		country: string,
		is_default: boolean,
		validated: boolean,
		created_at: timestamp_utc
	},
	user_sessions: {
		session_id: uuid_primary_key,
		user_id: uuid_foreign_key,
		jwt_token_hash: string,
		refresh_token_hash: string,
		ip_address: string,
		user_agent: string,
		device_info: json,
		created_at: timestamp_utc,
		expires_at: timestamp_utc,
		last_activity: timestamp_utc
	}
}

INDEXES=[
	"idx_users_email",
	"idx_users_account_status",
	"idx_user_sessions_user_id",
	"idx_user_addresses_user_id",
	"idx_user_preferences_user_id"
]
</data1>

<data2>
data_name="PRODUCT_INVENTORY_DATABASE"
data_type="relational"
source="postgresql_primary"
cache_layer="redis_primary_cache"
search_index="elasticsearch_product_index"
visibility="PRIVATE"
sync_interval="real_time"

SCHEMA={
	products: {
		product_id: uuid_primary_key,
		sku: string_unique,
		name: string,
		description: string_long,
		category_id: uuid_foreign_key,
		subcategory_id: uuid_foreign_key_nullable,
		brand_id: uuid_foreign_key,
		price: decimal_10_2,
		cost_price: decimal_10_2,
		profit_margin: decimal_5_2,
		currency: enum[USD, EUR, GBP, INR],
		rating: decimal_3_2,
		review_count: integer,
		image_urls: json_array,
		video_urls: json_array_nullable,
		specifications: json,
		tags: json_array,
		product_status: enum[active, inactive, discontinued],
		created_at: timestamp_utc,
		updated_at: timestamp_utc
	},
	inventory: {
		inventory_id: uuid_primary_key,
		product_id: uuid_foreign_key,
		warehouse_id: uuid_foreign_key,
		quantity_on_hand: integer,
		quantity_reserved: integer,
		quantity_available: integer_computed,
		reorder_level: integer,
		reorder_quantity: integer,
		stock_status: enum[in_stock, low_stock, out_of_stock, overstocked],
		last_stock_count_date: timestamp_utc,
		last_reorder_date: timestamp_utc_nullable,
		updated_at: timestamp_utc
	},
	product_categories: {
		category_id: uuid_primary_key,
		category_name: string,
		parent_category_id: uuid_foreign_key_nullable,
		description: string,
		image_url: string,
		display_order: integer,
		is_active: boolean,
		created_at: timestamp_utc
	},
	product_reviews: {
		review_id: uuid_primary_key,
		product_id: uuid_foreign_key,
		user_id: uuid_foreign_key,
		rating: integer_1_to_5,
		title: string,
		review_text: string_long,
		helpful_count: integer,
		unhelpful_count: integer,
		verified_purchase: boolean,
		created_at: timestamp_utc,
		updated_at: timestamp_utc
	}
}

INDEXES=[
	"idx_products_category_id",
	"idx_products_brand_id",
	"idx_inventory_product_id",
	"idx_inventory_warehouse_id",
	"idx_inventory_stock_status",
	"idx_product_reviews_product_id",
	"idx_product_reviews_user_id"
]
</data2>

<data3>
data_name="ORDER_TRANSACTION_DATABASE"
data_type="relational"
source="postgresql_primary"
visibility="PRIVATE"
backup_frequency="hourly"
compliance=["PCI_DSS", "SOC_2"]

SCHEMA={
	orders: {
		order_id: uuid_primary_key,
		user_id: uuid_foreign_key,
		order_number: string_unique_sequential,
		order_status: enum[pending, confirmed, processing, shipped, delivered, cancelled, refunded],
		order_date: timestamp_utc,
		expected_delivery_date: date,
		actual_delivery_date: date_nullable,
		subtotal: decimal_10_2,
		tax_amount: decimal_10_2,
		shipping_cost: decimal_10_2,
		discount_amount: decimal_10_2,
		total_amount: decimal_10_2,
		currency: enum[USD, EUR, GBP, INR],
		payment_status: enum[pending, authorized, captured, failed, refunded],
		shipping_address_id: uuid_foreign_key,
		billing_address_id: uuid_foreign_key,
		shipping_method: enum[standard, express, overnight, pickup],
		tracking_number: string_nullable,
		promo_code_applied: string_nullable,
		loyalty_points_used: integer,
		loyalty_points_earned: integer,
		order_notes: string_nullable,
		created_at: timestamp_utc,
		updated_at: timestamp_utc
	},
	order_items: {
		order_item_id: uuid_primary_key,
		order_id: uuid_foreign_key,
		product_id: uuid_foreign_key,
		quantity: integer,
		unit_price: decimal_10_2,
		discount_per_item: decimal_10_2,
		tax_per_item: decimal_10_2,
		line_total: decimal_10_2,
		created_at: timestamp_utc
	},
	payments: {
		payment_id: uuid_primary_key,
		order_id: uuid_foreign_key,
		payment_method: enum[credit_card, debit_card, paypal, wallet, bank_transfer],
		amount: decimal_10_2,
		currency: enum[USD, EUR, GBP, INR],
		status: enum[pending, authorized, captured, declined, refunded],
		stripe_transaction_id: string,
		last_4_digits: string,
		card_brand: enum[visa, mastercard, amex, discover],
		billing_email: string,
		created_at: timestamp_utc,
		processed_at: timestamp_utc_nullable
	},
	refunds: {
		refund_id: uuid_primary_key,
		order_id: uuid_foreign_key,
		payment_id: uuid_foreign_key,
		refund_reason: enum[customer_request, defective, wrong_item, duplicate, other],
		refund_amount: decimal_10_2,
		status: enum[pending, approved, processed, rejected],
		refund_method: enum[original_payment_method, store_credit],
		initiated_by: uuid_foreign_key,
		approved_by: uuid_foreign_key_nullable,
		created_at: timestamp_utc,
		processed_at: timestamp_utc_nullable
	}
}

INDEXES=[
	"idx_orders_user_id",
	"idx_orders_order_status",
	"idx_orders_order_date",
	"idx_order_items_order_id",
	"idx_payments_order_id",
	"idx_payments_status",
	"idx_refunds_order_id",
	"idx_refunds_status"
]
</data3>

<data4>
data_name="CUSTOMER_SUPPORT_DATABASE"
data_type="relational"
source="postgresql_primary"
visibility="PRIVATE"

SCHEMA={
	support_tickets: {
		ticket_id: uuid_primary_key,
		ticket_number: string_unique_sequential,
		user_id: uuid_foreign_key,
		order_id: uuid_foreign_key_nullable,
		category: enum[product_quality, delivery_issue, payment_problem, account_issue, return_request, general_inquiry, complaint],
		priority: enum[critical, high, medium, low],
		status: enum[open, in_progress, waiting_for_customer, resolved, closed, escalated],
		subject: string,
		description: string_long,
		sentiment_score: decimal_3_2_nullable,
		assigned_agent_id: uuid_foreign_key_nullable,
		resolution_notes: string_long_nullable,
		created_at: timestamp_utc,
		updated_at: timestamp_utc,
		resolved_at: timestamp_utc_nullable,
		closed_at: timestamp_utc_nullable
	},
	support_chat_messages: {
		message_id: uuid_primary_key,
		ticket_id: uuid_foreign_key,
		sender_type: enum[user, agent, bot],
		sender_id: uuid_foreign_key,
		message_content: string_long,
		message_type: enum[text, image, attachment],
		attachment_url: string_nullable,
		created_at: timestamp_utc
	},
	faq_articles: {
		article_id: uuid_primary_key,
		title: string,
		content: string_long,
		category: string,
		keywords: json_array,
		embedding_vector: float_array_384,
		helpful_count: integer,
		unhelpful_count: integer,
		updated_at: timestamp_utc,
		created_at: timestamp_utc
	}
}

INDEXES=[
	"idx_support_tickets_user_id",
	"idx_support_tickets_status",
	"idx_support_tickets_priority",
	"idx_support_tickets_assigned_agent_id",
	"idx_support_chat_messages_ticket_id",
	"idx_faq_articles_category"
]
</data4>

<data5>
data_name="ANALYTICS_METRICS_DATABASE"
data_type="timeseries"
source="elasticsearch_analytics"
timeseries_database="influxdb_or_prometheus"
visibility="PUBLIC_AGGREGATE_ONLY"
retention="90_days_detailed_2_years_aggregated"

METRICS_TRACKED={
	business_metrics: [daily_revenue, daily_orders, avg_order_value, conversion_rate, cart_abandonment_rate, repeat_purchase_rate],
	user_metrics: [daily_active_users, new_signups, user_retention_rate, churn_rate, lifetime_value],
	product_metrics: [top_selling_products, slow_moving_products, inventory_turnover, stockout_rate],
	system_metrics: [api_response_time, error_rate, uptime, cache_hit_ratio, database_query_time],
	customer_metrics: [satisfaction_score, support_response_time, refund_rate, complaint_rate]
}

</data5>

<data6>
data_name="MACHINE_LEARNING_MODELS_STORAGE"
data_type="unstructured"
source="aws_s3"
visibility="PRIVATE"

MODELS={
	recommendation_model: {
		model_name: "collaborative_filtering_matrix_factorization",
		version: "v3.2_production",
		framework: "tensorflow",
		auc_score: 0.92,
		ndcg_score: 0.89,
		update_frequency: "weekly",
		retraining_trigger: "performance_drop_or_scheduled"
	},
	demand_forecast_model: {
		model_name: "arima_time_series_forecasting",
		version: "v2.1_production",
		framework: "statsmodels",
		mape_score: 0.08,
		update_frequency: "daily",
		forecast_horizon: 90_days
	},
	fraud_detection_model: {
		model_name: "gradient_boosting_xgboost",
		version: "v1.5_production",
		framework: "xgboost",
		precision_score: 0.95,
		recall_score: 0.88,
		update_frequency: "weekly"
	},
	sentiment_analysis_model: {
		model_name: "bert_sentiment_classification",
		version: "v1.2_production",
		framework: "huggingface_transformers",
		accuracy_score: 0.93,
		update_frequency: "monthly"
	}
}

</data6>

</data>

// ==================== CACHING STRATEGY ====================

<cache>
cache_system="redis_distributed"
cache_replicas=3
cache_eviction_policy="allkeys-lru"
cache_max_memory="50GB"

CACHE_LAYERS={
	layer1_session_cache: {
		key_prefix: "session:",
		ttl: 3600,
		size_limit: "10GB",
		critical: true
	},
	layer2_product_cache: {
		key_prefix: "product:",
		ttl: 1800,
		size_limit: "15GB",
		critical: true
	},
	layer3_user_cache: {
		key_prefix: "user:",
		ttl: 3600,
		size_limit: "8GB",
		critical: true
	},
	layer4_recommendation_cache: {
		key_prefix: "rec:",
		ttl: 7200,
		size_limit: "10GB",
		critical: false
	},
	layer5_search_cache: {
		key_prefix: "search:",
		ttl: 900,
		size_limit: "7GB",
		critical: true
	}
}

CACHE_INVALIDATION_RULES={
	product_price_change: "invalidate_product_cache + search_cache",
	inventory_update: "invalidate_inventory_cache",
	user_preference_update: "invalidate_user_cache + recommendation_cache",
	order_placed: "invalidate_cart_cache + inventory_cache",
	promotion_applied: "invalidate_search_cache + product_cache"
}

</cache>

// ==================== AGENT 1: USER MANAGEMENT AND AUTHENTICATION AGENT ====================

<agent>

<agent1>
agent_name="User Management and Authentication Agent"
agent_id="AGENT_001_USER_AUTH"
priority="CRITICAL"
instance_count=5
data_access=["USER_PROFILE_DATABASE", "USER_SESSIONS", "AUDIT_LOGS"]
timeout_ms=5000

INTENT:"I WANT TO PROVIDE COMPREHENSIVE USER MANAGEMENT CAPABILITIES INCLUDING SECURE AUTHENTICATION, PROFILE MANAGEMENT, PREFERENCE TRACKING, AND GDPR-COMPLIANT DATA HANDLING SO THAT USERS CAN SAFELY ACCESS AND CONTROL THEIR ACCOUNT DATA"

DECLARE{
	SCOPE: global_application_level,
	RESPONSIBILITY: user_lifecycle_management,
	DATA_HANDLING: personal_identifiable_information_pii,
	SECURITY_LEVEL: critical
}

<DATA>
DATA_NAME="USER_AUTHENTICATION_FLOW"
DATA.PRIVATE()
ENVIRONMENT="@HOMEPAGE,@LOGIN_PAGE,@REGISTRATION_PAGE,@PROFILE_PAGE"{
	START

	// Registration Workflow
	REGISTRATION_INTERFACE={
		form_fields: [email, password, confirm_password, full_name, phone_number, terms_accepted],
		validation_rules: {
			email: "valid_email_format + unique_in_database",
			password: "min_12_chars + 1_uppercase + 1_number + 1_special_char",
			phone: "valid_phone_format_intl",
			terms: "must_be_accepted"
		},
		captcha: "recaptcha_v3_score_threshold_0.7"
	}

	// Login Workflow
	LOGIN_INTERFACE={
		form_fields: [email_or_username, password, remember_me_checkbox],
		authentication_methods: [email_password, oauth2_google, oauth2_facebook, saml_enterprise],
		mfa_options: [totp_authenticator, sms_otp, email_otp],
		session_management: "jwt_token_with_refresh_token"
	}

	// Profile Management
	PROFILE_INTERFACE={
		editable_fields: [full_name, phone_number, bio, profile_image, language, timezone],
		protected_fields: [email, user_id, created_at],
		sensitive_operations: [email_change_requires_verification, password_change_requires_current_password]
	}

	}
END

CONTEXT={
	LOAD.SESSION(from_redis_cache),
	LOAD.USER_RECORD(from_postgresql),
	LOAD.AUDIT_LOG(last_10_logins),
	LOAD.SECURITY_ALERTS(suspicious_activity)
}
</DATA>

RULE:"
	AUTHENTICATION_RULES:
		RULE-1: PASSWORDS MUST BE HASHED USING BCRYPT WITH 12 ROUNDS, NEVER STORE PLAIN TEXT
		RULE-2: JWT TOKENS EXPIRE IN 24 HOURS, REFRESH TOKENS IN 30 DAYS
		RULE-3: IMPLEMENT RATE LIMITING: MAX 5 FAILED LOGIN ATTEMPTS = 15 MINUTE LOCKOUT
		RULE-4: REQUIRE MFA (MULTI_FACTOR_AUTHENTICATION) FOR ALL ADMIN ACCOUNTS
		RULE-5: IMPLEMENT SESSION FIXATION PROTECTION: REGENERATE SESSION_ID AFTER LOGIN
		RULE-6: ENCRYPT SENSITIVE DATA IN TRANSIT USING TLS_1.3
		RULE-7: IMPLEMENT CSRF PROTECTION FOR ALL FORM SUBMISSIONS
		RULE-8: LOG ALL AUTHENTICATION EVENTS FOR AUDIT TRAIL (user, timestamp, ip, success/failure)

	DATA_PRIVACY_RULES:
		RULE-9: COMPLY WITH GDPR: OBTAIN EXPLICIT CONSENT FOR DATA COLLECTION
		RULE-10: IMPLEMENT RIGHT_TO_ACCESS: PROVIDE COMPLETE DATA EXPORT IN JSON FORMAT
		RULE-11: IMPLEMENT RIGHT_TO_BE_FORGOTTEN: DELETE ALL USER DATA WITHIN 30 DAYS OF REQUEST
		RULE-12: IMPLEMENT DATA_MINIMIZATION: COLLECT ONLY NECESSARY PERSONAL DATA
		RULE-13: ENCRYPT ALL PII DATA AT REST USING AES_256
		RULE-14: ANONYMIZE DATA AFTER 2 YEARS IF USER_INACTIVE
"

PROCESS:"
	WORKFLOW_REGISTRATION:
		STEP-1: RECEIVE USER INPUT FROM REGISTRATION FORM
		STEP-2: VALIDATE EMAIL FORMAT USING REGEX: ^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$
		STEP-3: CHECK EMAIL UNIQUENESS IN DATABASE - IF EXISTS, RETURN ERROR
		STEP-4: VALIDATE PASSWORD STRENGTH: min_12_chars AND contains(uppercase, lowercase, number, special_char)
		STEP-5: VERIFY PASSWORDS MATCH (password == confirm_password)
		STEP-6: VALIDATE PHONE NUMBER FORMAT USING LIBPHONENUMBER LIBRARY
		STEP-7: VERIFY TERMS_OF_SERVICE ACCEPTED
		STEP-8: VERIFY RECAPTCHA_V3 SCORE > 0.7 TO PREVENT BOTS
		STEP-9: HASH PASSWORD USING BCRYPT(password, salt_rounds=12)
		STEP-10: GENERATE UNIQUE USER_ID (UUID_V4)
		STEP-11: CREATE USER RECORD IN POSTGRESQL WITH status='email_verification_pending'
		STEP-12: GENERATE EMAIL_VERIFICATION_TOKEN (valid for 24 hours)
		STEP-13: SEND VERIFICATION_EMAIL WITH TOKEN AND CONFIRMATION_LINK
		STEP-14: LOG REGISTRATION_EVENT FOR ANALYTICS
		STEP-15: RETURN SUCCESS_RESPONSE TO CLIENT

	WORKFLOW_EMAIL_VERIFICATION:
		STEP-1: RECEIVE VERIFICATION_TOKEN FROM EMAIL_LINK
		STEP-2: QUERY USER_RECORD BY TOKEN
		STEP-3: CHECK IF TOKEN_EXPIRED: if (current_time > token_expiry_time) THEN REJECT
		STEP-4: IF VALID: UPDATE USER status='active', email_verified=true
		STEP-5: DELETE VERIFICATION_TOKEN (single_use)
		STEP-6: SEND WELCOME_EMAIL
		STEP-7: REDIRECT USER TO LOGIN PAGE

	WORKFLOW_LOGIN:
		STEP-1: RECEIVE EMAIL/USERNAME AND PASSWORD FROM LOGIN FORM
		STEP-2: QUERY USER RECORD BY EMAIL (case_insensitive)
		STEP-3: IF USER_NOT_FOUND: RETURN GENERIC_ERROR_MESSAGE (don't reveal user existence)
		STEP-4: IF USER status='deleted': RETURN ACCOUNT_DELETED_ERROR
		STEP-5: IF USER status='suspended': RETURN ACCOUNT_SUSPENDED_ERROR
		STEP-6: RETRIEVE STORED_PASSWORD_HASH FROM DATABASE
		STEP-7: COMPARE PROVIDED_PASSWORD WITH STORED_HASH USING BCRYPT.COMPARE()
		STEP-8: IF MISMATCH: INCREMENT failed_login_attempts COUNTER
		STEP-9: IF failed_login_attempts >= 5: LOCK ACCOUNT FOR 15 MINUTES, SEND SECURITY_ALERT_EMAIL
		STEP-10: IF MATCH: RESET failed_login_attempts TO 0
		STEP-11: IF MFA_ENABLED FOR USER:
			STEP-11A: SEND MFA_CHALLENGE (TOTP, SMS_OTP, or EMAIL_OTP)
			STEP-11B: WAIT FOR USER INPUT (timeout=5_minutes)
			STEP-11C: IF MFA_VALID: proceed; IF INVALID: REJECT_LOGIN
		STEP-12: IF NO_MFA: proceed directly
		STEP-13: GENERATE JWT_ACCESS_TOKEN (payload: {user_id, email, roles}, expiry: 24h)
		STEP-14: GENERATE REFRESH_TOKEN (payload: {user_id, token_version}, expiry: 30d)
		STEP-15: HASH TOKENS BEFORE STORAGE IN DATABASE
		STEP-16: STORE SESSION IN REDIS (key: session:{session_id})
		STEP-17: CREATE SESSION_RECORD IN POSTGRESQL WITH ip_address, user_agent, device_info
		STEP-18: SET SECURE_HTTPONLY_COOKIE FOR JWT_TOKEN (expires_at: 24h)
		STEP-19: SET SECURE_HTTPONLY_COOKIE FOR REFRESH_TOKEN (expires_at: 30d)
		STEP-20: LOG LOGIN_EVENT WITH timestamp, ip_address, device_info
		STEP-21: RETURN SUCCESS_RESPONSE WITH user_id, email, user_preferences

	WORKFLOW_TOKEN_REFRESH:
		STEP-1: RECEIVE REFRESH_TOKEN FROM CLIENT
		STEP-2: VERIFY TOKEN_SIGNATURE USING SECRET_KEY
		STEP-3: IF TOKEN_INVALID OR EXPIRED: REJECT_REQUEST
		STEP-4: EXTRACT user_id AND token_version FROM REFRESH_TOKEN
		STEP-5: QUERY USER_RECORD TO GET CURRENT_TOKEN_VERSION
		STEP-6: IF token_version_mismatch (user_issued_logout): REJECT_REQUEST (token_revoked)
		STEP-7: GENERATE NEW ACCESS_TOKEN WITH SAME PAYLOAD
		STEP-8: RETURN NEW_ACCESS_TOKEN TO CLIENT (refresh_token remains same)

	WORKFLOW_LOGOUT:
		STEP-1: RECEIVE SESSION_ID FROM CLIENT
		STEP-2: RETRIEVE SESSION_RECORD FROM REDIS
		STEP-3: INCREMENT token_version IN USER_RECORD (invalidates all refresh_tokens)
		STEP-4: DELETE SESSION FROM REDIS
		STEP-5: UPDATE SESSION status='logged_out' IN POSTGRESQL
		STEP-6: CLEAR HTTP_COOKIES (set max_age=0)
		STEP-7: LOG LOGOUT_EVENT
		STEP-8: RETURN SUCCESS_RESPONSE

	WORKFLOW_PROFILE_UPDATE:
		STEP-1: VERIFY JWT_TOKEN IS VALID
		STEP-2: EXTRACT user_id FROM TOKEN
		STEP-3: VALIDATE user_id MATCHES REQUEST_PARAMETER (prevent IDOR)
		STEP-4: RECEIVE PROFILE_UPDATE_DATA
		STEP-5: FOR EACH FIELD:
			STEP-5A: IF email: REQUIRE_VERIFICATION_EMAIL, KEEP OLD_EMAIL UNTIL NEW_VERIFIED
			STEP-5B: IF password: REQUIRE CURRENT_PASSWORD + NEW_PASSWORD + CONFIRMATION
			STEP-5C: IF phone: VALIDATE FORMAT, SEND OTP FOR VERIFICATION
			STEP-5D: IF profile_image: VALIDATE FILE_TYPE (jpg, png), FILE_SIZE < 5MB, SCAN_FOR_MALWARE
		STEP-6: UPDATE USER_RECORD IN DATABASE
		STEP-7: INVALIDATE RELATED_CACHES (user:{user_id})
		STEP-8: LOG PROFILE_UPDATE_EVENT WITH changed_fields
		STEP-9: RETURN UPDATED_PROFILE

	WORKFLOW_PASSWORD_RESET:
		STEP-1: RECEIVE EMAIL FROM PASSWORD_RESET_REQUEST
		STEP-2: QUERY USER BY EMAIL
		STEP-3: IF USER_NOT_FOUND: RETURN SUCCESS_MESSAGE (don't reveal user existence)
		STEP-4: IF USER_EXISTS: GENERATE RESET_TOKEN (valid for 1 hour only)
		STEP-5: STORE RESET_TOKEN IN DATABASE WITH expiry_time
		STEP-6: SEND PASSWORD_RESET_EMAIL WITH SECURE_LINK
		STEP-7: LOG PASSWORD_RESET_REQUEST WITH timestamp, ip_address
		STEP-8: RETURN SUCCESS_RESPONSE
		STEP-9: WHEN USER CLICKS_LINK:
			STEP-9A: VERIFY RESET_TOKEN IS_VALID AND_NOT_EXPIRED
			STEP-9B: DISPLAY PASSWORD_RESET_FORM
			STEP-9C: VALIDATE NEW_PASSWORD STRENGTH
			STEP-9D: HASH NEW_PASSWORD WITH BCRYPT
			STEP-9E: UPDATE USER password_hash IN DATABASE
			STEP-9F: DELETE RESET_TOKEN (single_use)
			STEP-9G: INCREMENT token_version (invalidate all sessions)
			STEP-9H: SEND CONFIRMATION_EMAIL
"

MANAGE:"
	REAL_TIME SESSION MONITORING: track active_sessions per user, alert if > 5 concurrent sessions
	SESSION CLEANUP: automatically delete expired_sessions from redis (daily_batch_job)
	LOGIN ATTEMPT MONITORING: track failed_login_attempts, implement progressive_delays (exponential_backoff)
	ACCOUNT_LOCKOUT_RECOVERY: allow user to unlock by email_verification or support_contact
	PREFERENCE CACHING: maintain user_preferences in redis for fast_access (ttl: 1_hour)
	AUDIT_LOGGING: every authentication_event logged for compliance and forensics
	GDPR_COMPLIANCE: track all_data_processing, maintain consent_records, enable_data_export
"

MAINTAIN:"
	DATA_CONSISTENCY: ensure email_uniqueness across all databases, reconcile postgresql + redis
	INTEGRITY_CHECKS: validate NO_NULL_REQUIRED_FIELDS, email_format, password_hash_integrity
	TOKEN_VALIDATION: verify JWT_SIGNATURE on every request, check_token_expiry, validate_claims
	SESSION_VALIDATION: ensure session_exists_in_redis before_granting_access
	PASSWORD_HASH_SECURITY: use bcrypt with consistent salt_rounds, never use md5_or_sha1
	GDPR_DATA_HANDLING: anonymize after 2_years inactivity, respect_data_deletion_requests
	COMPLIANCE_AUDIT: monthly_review of auth_failures, security_incidents, policy_violations
"

PROTECT:"
	RULE-1: NEVER STORE PLAIN_TEXT PASSWORDS, ALWAYS USE BCRYPT_HASHING
	RULE-2: IMPLEMENT ACCOUNT_LOCKOUT: 5 FAILED ATTEMPTS = 15 MINUTE LOCKOUT
	RULE-3: REQUIRE MFA FOR ALL ADMIN ACCOUNTS AND HIGH_VALUE_OPERATIONS
	RULE-4: IMPLEMENT SESSION_FIXATION_PROTECTION: regenerate_session_id_after_login
	RULE-5: USE HTTPONLY_SECURE_COOKIES TO PREVENT XSS_TOKEN_THEFT
	RULE-6: IMPLEMENT CSRF_TOKEN VALIDATION FOR ALL_STATE_CHANGING_OPERATIONS
	RULE-7: RATE_LIMIT_LOGIN_ENDPOINT: max 10 requests per_minute per_ip
	RULE-8: VALIDATE_IP_ADDRESS_CHANGES: alert if login_from_unusual_location
	RULE-9: IMPLEMENT_DEVICE_FINGERPRINTING: track device_characteristics, alert on_changes
	RULE-10: LOG_ALL_AUTHENTICATION_EVENTS: timestamp, ip, device, success/failure_reason
	RULE-11: ENCRYPT_ALL_DATA_IN_TRANSIT: enforce TLS_1.3_minimum
	RULE-12: ENCRYPT_ALL_SENSITIVE_DATA_AT_REST: aes_256_encryption
"

CREATE:"
	USER_RECORD WITH FIELDS: {user_id, email, password_hash, full_name, phone, status, roles, preferences, metadata, created_at, updated_at}
	SESSION_RECORD WITH FIELDS: {session_id, user_id, jwt_token_hash, refresh_token_hash, ip_address, user_agent, device_info, expires_at, created_at, last_activity}
	AUDIT_LOG WITH FIELDS: {log_id, user_id, action, timestamp, ip_address, result, details}
"

</agent1>

// ==================== AGENT 2: PRODUCT DISCOVERY AND SEARCH AGENT ====================

<agent2>
agent_name="Product Discovery and Search Agent"
agent_id="AGENT_002_PRODUCT_SEARCH"
priority="CRITICAL"
instance_count=8
data_access=["PRODUCT_INVENTORY_DATABASE", "ELASTICSEARCH_INDEX", "REDIS_SEARCH_CACHE"]
timeout_ms=5000

INTENT:"I WANT TO ENABLE POWERFUL PRODUCT DISCOVERY CAPABILITIES USING SEMANTIC SEARCH, INTELLIGENT FILTERING, AND REAL_TIME INVENTORY SYNCHRONIZATION SO USERS CAN QUICKLY FIND EXACTLY WHAT THEY'RE LOOKING FOR"

<DATA>
DATA_NAME="PRODUCT_SEARCH_ENGINE"
DATA.PUBLIC()
ENVIRONMENT="@SEARCH_PAGE,@CATEGORY_PAGE,@HOME_PAGE"{
	START

	SEARCH_INTERFACE={
		search_input: {
			query_string: "user_search_terms",
			autocomplete_enabled: true,
			typo_tolerance: "enabled",
			search_suggestions: "powered_by_elasticsearch_suggestions"
		},
		filters: {
			price_range: {min: 0, max: 100000, step: 100},
			categories: ["electronics", "fashion", "home", "books", "sports", "health"],
			brands: "dynamic_list_from_database",
			rating: {min: 1, max: 5, granularity: 0.5},
			stock_status: ["in_stock", "pre_order", "coming_soon"],
			seller_rating: {min: 1, max: 5},
			discount_percentage: "0_to_100"
		},
		sorting_options: {
			relevance: "bm25_score",
			price_asc: "lowest_to_highest",
			price_desc: "highest_to_lowest",
			rating: "highest_first",
			newest: "recently_added",
			most_sold: "sales_volume_descending"
		},
		pagination: {
			items_per_page: [20, 50, 100],
			max_page_size: 100,
			default_page_size: 20
		}
	}

	}
END

CONTEXT={
	LOAD.SEARCH_HISTORY(current_user_recent_searches),
	LOAD.TRENDING(popular_searches_today),
	LOAD.USER_PREFERENCES(preferred_categories),
	LOAD.INVENTORY_STATUS(real_time_stock_availability)
}
</DATA>

EXECUTE:"
	WORKFLOW_SEARCH_EXECUTION:
		STEP-1: RECEIVE search_query FROM USER AND VALIDATE (min_2_chars, max_100_chars)
		STEP-2: NORMALIZE query: lowercase, trim_whitespace, remove_special_chars
		STEP-3: APPLY spell_checker: detect_typos_and_suggest_corrections
		STEP-4: CHECK REDIS_SEARCH_CACHE FOR IDENTICAL_QUERY (within_last_24_hours)
		STEP-5: IF CACHE_HIT: VERIFY INVENTORY_STATUS IS_CURRENT (< 30_min_old)
			STEP-5A: IF CURRENT: return_cached_results
			STEP-5B: IF STALE: update_inventory_status_only
		STEP-6: IF CACHE_MISS: EXECUTE ELASTICSEARCH_QUERY
		STEP-7: BUILD ELASTICSEARCH_QUERY WITH:
			- query: multi_match {query_string, fields: [name, description, tags], fuzziness: AUTO}
			- minimum_should_match: 75%
			- boost: {name: 3.0, tags: 2.0, description: 1.0}
		STEP-8: APPLY FILTER_CONSTRAINTS:
			- price_range: {gte: min_price, lte: max_price}
			- category: {in: selected_categories}
			- brand: {in: selected_brands}
			- rating: {gte: min_rating}
			- stock_status: {in: [in_stock, pre_order]}
		STEP-9: APPLY SORT_ORDER BASED_ON selected_sort_by:
			- relevance: _score_descending
			- price: price_ascending_or_descending
			- rating: rating_count_descending
			- newest: created_at_descending
		STEP-10: FETCH TOTAL_HITS COUNT
		STEP-11: IMPLEMENT PAGINATION: skip = (page_number - 1) * items_per_page
		STEP-12: EXECUTE QUERY WITH TIMEOUT: 5_seconds (abort_if_exceeds)
		STEP-13: RETRIEVE PRODUCT_DATA: {product_id, name, price, rating, image_url, brand, category}
		STEP-14: FETCH REAL_TIME INVENTORY_STATUS FROM REDIS_CACHE (refresh_every_30_sec)
		STEP-15: CALCULATE relevance_score FOR EACH PRODUCT:
			score = (elasticsearch_score * 0.7) + (user_preference_boost * 0.2) + (popularity_score * 0.1)
		STEP-16: LOAD PRODUCT_IMAGES FROM CDN_CLOUDFRONT WITH LAZY_LOADING
		STEP-17: GENERATE FACET_COUNTS FOR EACH_FILTER (how_many_results_per_category, price_range, etc)
		STEP-18: CACHE RESULTS IN REDIS WITH ttl=1_hour
		STEP-19: LOG SEARCH_EVENT FOR ANALYTICS: {query, filters, result_count, timestamp}
		STEP-20: RETURN JSON_RESPONSE WITH: {results, total_count, page_info, facets, suggestions}

	WORKFLOW_AUTOCOMPLETE_SUGGESTIONS:
		STEP-1: RECEIVE PARTIAL_QUERY (user_typing_search)
		STEP-2: QUERY ELASTICSEARCH SUGGESTION_INDEX (3_character_minimum)
		STEP-3: RETRIEVE TOP_10_SUGGESTIONS RANKED_BY:
			- popularity (search_frequency)
			- product_existence (filter_invalid_suggestions)
		STEP-4: APPLY spell_correction IF_TYPO_DETECTED
		STEP-5: RETURN SUGGESTIONS WITH_RESULT_COUNT FOR EACH

	WORKFLOW_CATEGORY_BROWSING:
		STEP-1: RECEIVE category_id FROM USER
		STEP-2: QUERY ELASTICSEARCH WITH filter: {category_id: selected_category}
		STEP-3: RETRIEVE PRODUCTS_IN_CATEGORY SORTED_BY trending_score
		STEP-4: APPLY DEFAULT_SORT: {newest_first, then_rating, then_popularity}
		STEP-5: GENERATE FACETS FOR: {price_ranges, brands, ratings}
		STEP-6: RETURN PAGINATED_RESULTS

	WORKFLOW_ADVANCED_FILTERING:
		STEP-1: RECEIVE FILTER_PARAMETERS FROM USER
		STEP-2: VALIDATE EACH_FILTER_VALUE:
			- price: min >= 0, max <= max_product_price
			- rating: 1 to 5 in 0.5 increments
			- category: must_exist_in_category_table
		STEP-3: BUILD ELASTICSEARCH_QUERY WITH_ALL_FILTERS
		STEP-4: APPLY RANGE_QUERIES FOR_NUMERIC_FILTERS
		STEP-5: APPLY TERM_QUERIES FOR_CATEGORICAL_FILTERS
		STEP-6: EXECUTE COMBINED_QUERY
		STEP-7: RETURN FILTERED_RESULTS WITH_DYNAMIC_FACET_UPDATES
"

MANAGE:"
	SEARCH_PERFORMANCE: track query_response_time, alert if > 500ms
	CACHE_OPTIMIZATION: maintain cache_hit_ratio > 70%, monitor cache_memory_usage
	INDEX_HEALTH: daily elasticsearch index verification, detect_shards_issues
	SEARCH_ANALYTICS: track top_searches, zero_result_queries, click_through_rate
	INVENTORY_SYNC: ensure elasticsearch reflects current_inventory within 30_seconds
	TRENDING_COMPUTATION: hourly update of trending_products and popular_searches
"

MAINTAIN:"
	INDEX_CONSISTENCY: verify elasticsearch_index matches postgresql_database nightly
	DATA_QUALITY: detect_duplicate_products, validate_product_specifications
	FACET_ACCURACY: verify facet_counts match_actual_results
	SEARCH_TIMEOUT: implement query_abort if > 5_seconds execution
	INVENTORY_FRESHNESS: flag_stale_inventory if last_update > 1_hour_old
"

PROTECT:"
	RULE-1: SANITIZE search_query TO PREVENT ELASTICSEARCH_INJECTION_ATTACKS
	RULE-2: RATE_LIMIT search_api: 100 requests per_minute per_user_ip
	RULE-3: TIMEOUT all_queries: abort_if > 5_seconds
	RULE-4: FILTER_OUT: discontinued_products, out_of_stock_items, hidden_products
	RULE-5: IMPLEMENT_QUERY_COMPLEXITY_LIMITS to prevent_dos_attacks
"

CREATE:"
	SEARCH_RESULT WITH FIELDS: {product_id, name, price, rating, image_url, relevance_score, in_stock}
	SEARCH_LOG WITH FIELDS: {search_id, query, filters, result_count, timestamp, user_id}
"

</agent2>

// ==================== AGENT 3: INTELLIGENT RECOMMENDATION ENGINE ====================

<agent3>
agent_name="Intelligent Recommendation Engine Agent"
agent_id="AGENT_003_RECOMMENDATIONS"
priority="HIGH"
instance_count=6
data_access=["PRODUCT_INVENTORY_DATABASE", "USER_DATABASE", "USER_BEHAVIOR_LOGS", "ML_MODELS"]
timeout_ms=3000

INTENT:"I WANT TO GENERATE HIGHLY PERSONALIZED PRODUCT RECOMMENDATIONS USING COLLABORATIVE FILTERING, CONTENT_BASED_FILTERING, AND MACHINE_LEARNING MODELS TO MAXIMIZE CONVERSION_RATE, AVERAGE_ORDER_VALUE, AND CUSTOMER_SATISFACTION"

<DATA>
DATA_NAME="RECOMMENDATION_ENGINE"
DATA.PRIVATE()
ENVIRONMENT="@HOME_PAGE,@PRODUCT_PAGE,@CHECKOUT_PAGE,@EMAIL_CAMPAIGNS"{
	START

	RECOMMENDATION_CONTEXT={
		user_history: {
			purchases: "last_12_months",
			browsing_history: "last_30_days",
			cart_history: "last_30_days",
			wishlist_items: "all_time",
			search_queries: "last_30_days"
		},
		similarity_metrics: {
			user_user_similarity: "cosine_similarity_on_purchase_vectors",
			product_product_similarity: "cosine_similarity_on_attribute_vectors",
			user_profile_matching: "demographic_and_preference_alignment"
		},
		ranking_factors: {
			collaborative_score: "weight_0.4",
			content_similarity_score: "weight_0.3",
			popularity_score: "weight_0.15",
			profit_margin_boost: "weight_0.1",
			personalization_score: "weight_0.05"
		},
		diversity_constraints: {
			max_same_category: 0.3,
			max_same_brand: 0.2,
			price_diversity: "recommend_across_price_ranges"
		}
	}

	}
END

CONTEXT={
	LOAD.USER_PROFILE(demographics, preferences, loyalty_tier),
	LOAD.PURCHASE_HISTORY(products, categories, prices, dates),
	LOAD.BROWSING_BEHAVIOR(viewed_products, view_duration, click_patterns),
	LOAD.SIMILAR_USERS(k_nearest_neighbors=50),
	LOAD.TRENDING(popular_products_today, seasonal_trends),
	LOAD.INVENTORY(product_availability, stock_levels)
}
</DATA>

PROCESS:"
	DECISION_TREE_NEW_VS_RETURNING_USER:
		IF user_is_new (no_purchase_history) THEN
			BRANCH: recommendation_strategy_for_new_user
		ELSE IF user_is_returning (purchase_history_exists) THEN
			BRANCH: recommendation_strategy_for_returning_user
		ELSE IF user_is_inactive (no_activity_30_days) THEN
			BRANCH: recommendation_strategy_for_reengagement

	WORKFLOW_RECOMMENDATION_FOR_NEW_USER:
		STEP-1: RETRIEVE user_preferences (category_preferences, price_range, brands)
		STEP-2: IF preferences_provided:
			STEP-2A: QUERY PRODUCTS MATCHING_USER_PREFERENCES
			STEP-2B: SORT BY trending_score AND rating
			STEP-2C: MIX: 70% relevant_to_preferences + 30% popular_across_platform
		STEP-3: ELSE (no_preferences):
			STEP-3A: LOAD GLOBAL_TRENDING_PRODUCTS (top_100_by_sales_volume)
			STEP-3B: LOAD MOST_REVIEWED_PRODUCTS (high_rating_and_many_reviews)
			STEP-3C: MIX: 60% trending + 40% highly_rated
		STEP-4: APPLY DIVERSITY_FILTER: ensure >= 5_different_categories
		STEP-5: RANK BY relevance_score
		STEP-6: RETURN TOP_20_RECOMMENDATIONS

	WORKFLOW_RECOMMENDATION_FOR_RETURNING_USER:
		STEP-1: RETRIEVE user_purchase_history (products, categories, price_points, dates)
		STEP-2: IDENTIFY user_preferences FROM_HISTORICAL_DATA:
			- preferred_categories: most_purchased_categories
			- preferred_price_range: average_price_of_purchases
			- preferred_brands: brands_user_has_bought_from
		STEP-3: CALCULATE USER_EMBEDDING (user_vector) FROM_PURCHASE_HISTORY
		STEP-4: FIND K_NEAREST_NEIGHBORS (k=50) USING_COSINE_SIMILARITY
		STEP-5: EXTRACT top_products_from_neighbors (products_they_bought_that_user_hasn't)
		STEP-6: APPLY COLLABORATIVE_FILTERING:
			STEP-6A: FOR EACH_NEIGHBOR: weight_by_similarity_score
			STEP-6B: AGGREGATE: sum_of_weighted_neighbor_purchases
			STEP-6C: SCORE_EACH_CANDIDATE_PRODUCT by_aggregated_weight
		STEP-7: APPLY CONTENT_BASED_FILTERING:
			STEP-7A: CALCULATE PRODUCT_EMBEDDINGS (from_attributes, categories, reviews)
			STEP-7B: FIND_SIMILAR_PRODUCTS (cosine_similarity > 0.7)
			STEP-7C: SCORE: average_similarity_to_user_purchased_items
		STEP-8: COMBINE_SCORES:
			final_score = (0.4 × collaborative_score) + (0.3 × content_similarity) + (0.15 × popularity) + (0.1 × profit_margin) + (0.05 × personalization)
		STEP-9: APPLY FILTERS: remove_already_purchased, remove_out_of_stock, remove_discontinued
		STEP-10: APPLY DIVERSITY_CONSTRAINTS:
			- max_category_same: 30%
			- max_brand_same: 20%
			- price_range_diversity: recommend_across_all_price_tiers
		STEP-11: APPLY PRICE_RANGE_FILTER:
			- 50% within_user_average_price ±20%
			- 30% in_higher_price_range (aspirational_products)
			- 20% in_lower_price_range (budget_options)
		STEP-12: RANK_FINAL_CANDIDATES BY_SCORE
		STEP-13: RETURN TOP_20_RECOMMENDATIONS

	WORKFLOW_REENGAGEMENT_STRATEGY:
		STEP-1: USER_INACTIVE > 30_DAYS
		STEP-2: RETRIEVE user_last_purchases (categories, brands, price_points)
		STEP-3: FIND NEW_PRODUCTS IN_USER_PREFERRED_CATEGORIES (added_after_last_activity)
		STEP-4: LOAD SPECIAL_OFFERS_AND_DISCOUNTS
		STEP-5: MIX: 50% new_in_preferred_categories + 30% discounted_items + 20% trending_overall
		STEP-6: COMPOSE REENGAGEMENT_EMAIL with_personalized_recommendations
		STEP-7: ADD INCENTIVE: exclusive_discount_code or_loyalty_points_bonus
"

MANAGE:"
	MODEL_PERFORMANCE: weekly comparison of forecast_vs_actual_conversion, target_ctr >= 5%
	A_B_TESTING: run continuous tests (70% model_v3.2 vs 30% model_v3.1)
	CACHE_STRATEGY: store top_20_recommendations per_user in_redis (ttl: 2_hours)
	RECOMMENDATION_DIVERSITY: ensure recommendation_diversity_score >= 0.7
	PROFIT_OPTIMIZATION: weekly adjust profit_margin_weight based_on_inventory_levels
"

MAINTAIN:"
	MODEL_DRIFT_DETECTION: monthly check model_performance >= baseline - 5%, retrain_if_below
	COLD_START_SOLUTION: for_new_products (< 7_days_old), increase_popularity_weight to 40%
	BIAS_DETECTION: audit_monthly that_no_brand exceeds 30% of_recommendations
	DATA_QUALITY: validate no_null_values in user_preferences, filter_out_bot_behavior
	EXPLAINABILITY: log_reason for_each_recommendation (similar_user_bought, trending, collaborative_score, item_similarity)
"

PROTECT:"
	RULE-1: NEVER_RECOMMEND products_user_already_purchased within_90_days
	RULE-2: NEVER_RECOMMEND out_of_stock or_discontinued_products
	RULE-3: FILTER_RECOMMENDATIONS by_user_age (no_adult_products_to_minors)
	RULE-4: IMPLEMENT_RECOMMENDATION_DIVERSITY: max_20%_from_same_brand
	RULE-5: BLOCK_RECOMMENDATIONS from_sellers with_fraud_flag or_complaint_ratio > 5%
	RULE-6: TIMEOUT_RECOMMENDATION_GENERATION if > 3_seconds
"

CREATE:"
	RECOMMENDATION_LIST WITH FIELDS: {product_id, score, rank, reason, estimated_conversion_probability}
	RECOMMENDATION_EVENT LOG WITH FIELDS: {timestamp, user_id, recommended_product_ids, clicked, purchased, conversion_indicator}
"

</agent3>

// ==================== AGENT 4: SHOPPING CART AND CHECKOUT AGENT ====================

<agent4>
agent_name="Shopping Cart and Checkout Agent"
agent_id="AGENT_004_CART_CHECKOUT"
priority="CRITICAL"
instance_count=10
data_access=["SHOPPING_CART_SESSION", "PRODUCT_INVENTORY_DATABASE", "PAYMENT_GATEWAY", "ORDER_DATABASE"]
timeout_ms=8000

INTENT:"I WANT TO PROVIDE A SEAMLESS, SECURE SHOPPING CART EXPERIENCE WITH DYNAMIC PRICING, AUTOMATIC TAX CALCULATION, PROMOTIONAL CODE APPLICATION, AND PCI_DSS COMPLIANT PAYMENT PROCESSING"

<DATA>
DATA_NAME="SHOPPING_CART_AND_CHECKOUT_FLOW"
DATA.PRIVATE()
ENVIRONMENT="@CART_PAGE,@CHECKOUT_PAGE,@PAYMENT_PAGE"{
	START

	CART_STRUCTURE={
		cart_id: "unique_per_user_session",
		storage: "redis_for_active_session + postgresql_for_persistence",
		items: [{
			product_id,
			product_name,
			quantity,
			unit_price,
			line_total,
			discount_per_item,
			timestamp_added
		}],
		cart_metadata: {
			user_id,
			session_id,
			created_at,
			last_updated_at,
			expiry_time_7_days
		}
	}

	CHECKOUT_FLOW={
		step1_shipping_address: "user_enters_or_selects_address",
		step2_shipping_method: "user_selects_shipping_speed",
		step3_promo_code: "user_applies_discount_code",
		step4_payment_method: "user_enters_payment_info",
		step5_order_review: "user_reviews_and_confirms",
		step6_payment_processing: "system_processes_payment",
		step7_order_confirmation: "system_confirms_order"
	}

	}
END

CONTEXT={
	LOAD.CART_ITEMS(from_redis_session),
	LOAD.PRODUCT_DETAILS(prices, taxes, inventory),
	LOAD.ACTIVE_PROMOTIONS(current_date),
	LOAD.APPLICABLE_TAX_RATES(by_location),
	LOAD.SHIPPING_OPTIONS(by_weight_and_location),
	LOAD.USER_PAYMENT_METHODS(saved_cards)
}
</DATA>

EXECUTE:"
	WORKFLOW_ADD_TO_CART:
		STEP-1: RECEIVE product_id AND quantity FROM_CLIENT
		STEP-2: VALIDATE product_id EXISTS IN_INVENTORY
		STEP-3: QUERY PRODUCT_DETAILS: {price, name, image_url, specifications}
		STEP-4: VERIFY stock_available >= requested_quantity
		STEP-5: CALCULATE line_total = quantity × current_price
		STEP-6: QUERY ACTIVE_DISCOUNTS FOR_THIS_PRODUCT (if_any)
		STEP-7: RETRIEVE CART FROM REDIS (key: cart:{session_id})
		STEP-8: IF product_already_in_cart:
			STEP-8A: UPDATE quantity += requested_quantity
			STEP-8B: VALIDATE NEW_QUANTITY <= stock_available
		STEP-9: ELSE: ADD_NEW_ITEM_TO_CART
		STEP-10: RESERVE INVENTORY: reserve_quantity += requested_quantity (10_minute_hold)
		STEP-11: RECALCULATE cart_subtotal = sum(line_total for_all_items)
		STEP-12: UPDATE CART IN REDIS WITH new_items AND expiry_time (7_days)
		STEP-13: PERSIST CART_BACKUP TO_POSTGRESQL (for_abandoned_cart_recovery)
		STEP-14: TRIGGER CACHE_INVALIDATION FOR related_caches
		STEP-15: LOG ADD_TO_CART_EVENT FOR_ANALYTICS
		STEP-16: RETURN UPDATED_CART: {item_count, subtotal, estimated_total}

	WORKFLOW_REMOVE_FROM_CART:
		STEP-1: RECEIVE product_id FROM_CLIENT
		STEP-2: RETRIEVE CART FROM_REDIS
		STEP-3: IF product_in_cart: REMOVE_ITEM
		STEP-4: RELEASE INVENTORY_RESERVATION FOR_THIS_PRODUCT
		STEP-5: RECALCULATE cart_subtotal
		STEP-6: UPDATE CART IN_REDIS_AND_POSTGRESQL
		STEP-7: LOG REMOVE_FROM_CART_EVENT

	WORKFLOW_APPLY_PROMO_CODE:
		STEP-1: RECEIVE promo_code FROM_USER
		STEP-2: VALIDATE code_format (6-12 alphanumeric_characters)
		STEP-3: QUERY PROMOTIONS TABLE BY_CODE
		STEP-4: IF code_not_found: RETURN ERROR_MESSAGE
		STEP-5: VERIFY code_is_active: current_date BETWEEN start_date AND end_date
		STEP-6: VERIFY code_not_expired
		STEP-7: CHECK user_hasn't_used_this_code_before (for_single_use_codes)
		STEP-8: VERIFY discount_minimum_purchase <= cart_subtotal
		STEP-9: VERIFY user_meets_all_conditions:
			- loyalty_tier_requirement: user_tier >= required_tier
			- product_category_restriction: products_in_cart MATCH_eligible_categories
			- first_time_buyer_only: if_applicable
		STEP-10: CALCULATE discount_amount:
			IF discount_type = 'PERCENTAGE': discount = cart_subtotal × discount_percentage / 100
			ELSE IF discount_type = 'FIXED': discount = discount_amount
		STEP-11: APPLY discount_cap IF_SPECIFIED (e.g., max_discount = $50)
		STEP-12: VERIFY discount_amount <= max_discount
		STEP-13: UPDATE CART with_promo_code AND discount_amount
		STEP-14: IF code_is_single_use: MARK_AS_USED for_this_user
		STEP-15: RECALCULATE cart_total = subtotal + tax + shipping - discount
		STEP-16: LOG PROMO_CODE_APPLICATION: {user_id, code, discount_amount}
		STEP-17: RETURN UPDATED_CART with_new_total

	WORKFLOW_CALCULATE_SHIPPING:
		STEP-1: RECEIVE delivery_address FROM_USER
		STEP-2: EXTRACT shipping_location: {city, state, postal_code, country}
		STEP-3: VALIDATE address_format USING address_validation_api
		STEP-4: DETERMINE shipping_zone FROM_POSTAL_CODE
		STEP-5: CALCULATE total_weight = sum(product_weight × quantity FOR_EACH_ITEM)
		STEP-6: DETERMINE_DIMENSIONS: {length, width, height, dimensional_weight}
		STEP-7: QUERY SHIPPING_RATES_TABLE FOR: {zone, weight_range, shipping_method}
		STEP-8: CALCULATE shipping_cost FOR_EACH_METHOD:
			- standard_shipping (5-7_days): $5 + $0.50 per_pound
			- express_shipping (2-3_days): $15 + $1.00 per_pound
			- overnight_shipping (next_day): $30 + $2.00 per_pound
		STEP-9: CHECK_FOR_FREE_SHIPPING:
			IF cart_subtotal > $100: free_standard_shipping
			ELSE IF user_loyalty_tier = 'PLATINUM': free_shipping_any_method
			ELSE: charged_shipping
		STEP-10: APPLY carrier_surcharges: {residential=+$2.50, remote=+$10}
		STEP-11: UPDATE CART WITH_SHIPPING_OPTIONS
		STEP-12: RETURN SHIPPING_OPTIONS WITH_COSTS

	WORKFLOW_CALCULATE_TAX:
		STEP-1: RECEIVE delivery_address FROM_USER
		STEP-2: EXTRACT tax_jurisdiction: {state, province, country}
		STEP-3: QUERY_TAX_RATES_TABLE FOR_JURISDICTION
		STEP-4: FOR_EACH_ITEM_IN_CART:
			STEP-4A: DETERMINE product_tax_category (taxable, tax_exempt, reduced_rate)
			STEP-4B: IF product_digital: tax_rate = 0% (digital_products_tax_exempt_most_states)
			STEP-4C: IF product_gift_card: tax_rate = 0%
			STEP-4D: IF product_groceries: tax_rate = reduced_rate (4%)
			STEP-4E: ELSE: tax_rate = standard_rate (8%)
		STEP-5: CALCULATE tax_amount:
			subtotal_taxable = sum(price FOR_taxable_items_only)
			tax_amount = subtotal_taxable × applicable_tax_rate
		STEP-6: HANDLE multi_state_shipments: calculate_tax_for_each_state
		STEP-7: UPDATE CART WITH_TAX_BREAKDOWN: {tax_per_item, total_tax}
		STEP-8: RETURN TAX_DETAILS

	WORKFLOW_FINAL_PRICE_CALCULATION:
		STEP-1: CALCULATE: final_total = subtotal + tax + shipping - discount
		STEP-2: ROUND to_2_decimals USING banker's_rounding
		STEP-3: VERIFY: final_total > 0 (don't allow negative)
		STEP-4: RETURN BREAKDOWN: {subtotal, tax, shipping, discount, total}

	WORKFLOW_PAYMENT_PROCESSING:
		STEP-1: RETRIEVE_CART FROM_REDIS
		STEP-2: VALIDATE cart_not_empty: item_count > 0
		STEP-3: VALIDATE final_total > 0
		STEP-4: RECEIVE payment_details FROM_CLIENT: {card_number, expiry, cvv}
		STEP-5: VALIDATE payment_method_format
		STEP-6: GENERATE idempotency_key = hash(user_id + timestamp + random)
		STEP-7: ENCRYPT payment_data USING end_to_end_encryption (client_side_encryption)
		STEP-8: VERIFY PCI_DSS_COMPLIANCE: never_log_full_card_numbers
		STEP-9: CREATE order_record IN_DATABASE WITH status='PAYMENT_PENDING'
		STEP-10: SEND_PAYMENT_REQUEST TO_STRIPE_API WITH:
			- amount: final_total × 100 (in_cents)
			- currency: order_currency
			- source: tokenized_card_token
			- idempotency_key: for_idempotency
			- metadata: {order_id, user_id}
		STEP-11: IMPLEMENT TIMEOUT: wait_max_30_seconds_for_stripe_response
		STEP-12: HANDLE stripe_response:
			IF status = 'succeeded':
				STEP-12A: UPDATE order_status = 'CONFIRMED'
				STEP-12B: UPDATE payment_status = 'CAPTURED'
				STEP-12C: EMIT order_created_event (trigger_fulfillment_workflow)
				STEP-12D: PROCEED TO order_confirmation
			ELSE IF status = 'requires_action':
				STEP-12E: IMPLEMENT 3D_SECURE flow (return_authentication_url)
			ELSE IF status = 'declined':
				STEP-12F: UPDATE payment_status = 'DECLINED'
				STEP-12G: LOG decline_reason
				STEP-12H: RETURN error_to_user, allow_retry
		STEP-13: ON payment_success:
			STEP-13A: UPDATE INVENTORY: quantity_on_hand -= ordered_quantity (for_each_item)
			STEP-13B: RELEASE inventory_reservation
			STEP-13C: CALCULATE loyalty_points_earned = order_total × 0.01 (1_point_per_dollar)
			STEP-13D: UPDATE user_loyalty_balance += loyalty_points_earned
			STEP-13E: CLEAR CART FROM_REDIS
			STEP-13F: SEND CONFIRMATION_EMAIL TO_USER WITH order_details
			STEP-13G: SEND ORDER_NOTIFICATION TO_WAREHOUSE (trigger_fulfillment)
		STEP-14: ON payment_failure:
			STEP-14A: RELEASE inventory_reservation (inventory becomes_available_again)
			STEP-14B: UPDATE order_status = 'PAYMENT_FAILED'
			STEP-14C: ALLOW user_to_retry (max_3_attempts)
			STEP-14D: LOG failed_payment_attempt WITH_error_details
"

MANAGE:"
	CART_PERSISTENCE: save_active_carts_in_redis + postgresql_backup_every_10_min
	ABANDONED_CART_RECOVERY: send_reminder_email if_cart_abandoned > 24_hours
	PAYMENT_RETRY_LOGIC: implement_exponential_backoff for_failed_payment_retries
	INVENTORY_RESERVATION: hold_inventory for_10_min after_item_added_to_cart
	MONITOR: cart_conversion_rate, average_order_value, cart_abandonment_rate
	PAYMENT_MONITORING: track_failed_payments_by_decline_reason, alert_on_anomalies
"

MAINTAIN:"
	PRICE_CALCULATION_INTEGRITY: verify final_total = subtotal + tax + shipping - discount
	INVENTORY_CONSISTENCY: ensure_inventory_in_redis_matches_postgresql_before_payment
	PCI_COMPLIANCE: never_log_full_card_numbers, only_store_last_4_digits
	MONEY_PRECISION: use_decimal(10,2)_data_type for_all_monetary_values (prevent_rounding_errors)
	REFUND_INTEGRITY: track_all_refund_transactions + reverse_inventory_adjustments
"

PROTECT:"
	RULE-1: IMPLEMENT payment_amount_validation: if_amount > $999,999_flag_as_suspicious
	RULE-2: DETECT velocity_abuse: block_if_same_user makes > 5_purchases_in_5_minutes
	RULE-3: IMPLEMENT 3D_SECURE for_cards not_in_cvv_verification_whitelist
	RULE-4: VALIDATE billing_address MATCHES_card_address (loose_match_acceptable)
	RULE-5: IMPLEMENT fraud_score_calculation: if_score > threshold, require_manual_review
	RULE-6: NEVER_STORE full_payment_data IN_APPLICATION_LOGS
	RULE-7: ENFORCE TLS_1.3 for_all_payment_communication
	RULE-8: RATE_LIMIT checkout_api: 50_requests_per_minute_per_user
"

CREATE:"
	ORDER RECORD WITH FIELDS: {order_id, user_id, order_number, order_date, items, subtotal, tax, shipping, discount, total, status, payment_method, tracking_number}
	PAYMENT_TRANSACTION LOG WITH FIELDS: {transaction_id, order_id, amount, status, timestamp, stripe_transaction_id, last_4_digits}
"

</agent4>

// ==================== AGENT 5: INVENTORY MANAGEMENT AND OPTIMIZATION AGENT ====================

<agent5>
agent_name="Inventory Management and Optimization Agent"
agent_id="AGENT_005_INVENTORY"
priority="HIGH"
instance_count=4
data_access=["PRODUCT_INVENTORY_DATABASE", "WAREHOUSE_SYSTEM", "SUPPLIER_API", "ANALYTICS"]
timeout_ms=10000

INTENT:"I WANT TO MAINTAIN OPTIMAL INVENTORY LEVELS BY PREDICTING DEMAND, MONITORING STOCK IN REAL_TIME, AUTOMATICALLY TRIGGERING REORDERS, AND OPTIMIZING WAREHOUSE SPACE UTILIZATION"

<DATA>
DATA_NAME="INVENTORY_MANAGEMENT_SYSTEM"
DATA.SCOPE="INTERNAL_OPERATIONS"
ENVIRONMENT="@WAREHOUSE_SYSTEM,@INVENTORY_DASHBOARD"{
	START

	INVENTORY_TRACKING={
		warehouse_locations: ["warehouse_us_east", "warehouse_eu_west", "warehouse_ap_southeast"],
		stock_movements: {
			inbound: "purchase_orders_from_suppliers",
			outbound: "orders_shipped_to_customers",
			transfers: "inventory_moves_between_warehouses",
			adjustments: "physical_count_corrections, damages, returns"
		},
		stock_statuses: ["in_stock", "low_stock", "out_of_stock", "overstocked", "on_order", "discontinued"]
	}

	}
END

CONTEXT={
	LOAD.SALES_VELOCITY(last_30_days),
	LOAD.SEASONAL_DEMAND_FORECAST(next_90_days),
	LOAD.SUPPLIER_CAPACITY_CONSTRAINTS(),
	LOAD.WAREHOUSE_CAPACITY(available_space),
	LOAD.CURRENT_ORDERS_IN_TRANSIT()
}
</DATA>

PROCESS:"
	WORKFLOW_REAL_TIME_STOCK_MONITORING:
		STEP-1: EVERY_5_MINUTES:
			STEP-1A: QUERY INVENTORY_TABLE FOR_ALL_PRODUCTS
			STEP-1B: CALCULATE quantity_available = quantity_on_hand - quantity_reserved
			STEP-1C: CLASSIFY stock_status FOR_EACH_PRODUCT:
				WHERE quantity_available > (reorder_level × 1.5): status = 'OVERSTOCKED'
				WHERE quantity_available BETWEEN reorder_level AND (reorder_level × 1.5): status = 'HEALTHY'
				WHERE quantity_available < reorder_level: status = 'LOW_STOCK'
				WHERE quantity_available = 0: status = 'OUT_OF_STOCK'
		STEP-2: UPDATE stock_status IN_INVENTORY_TABLE
		STEP-3: FOR products_with_status_CHANGE:
			STEP-3A: UPDATE_CACHE (invalidate_product_cache)
			STEP-3B: IF status_changed_to_OUT_OF_STOCK: emit_stockout_alert
			STEP-3C: IF status_changed_to_LOW_STOCK: trigger_reorder_evaluation

	WORKFLOW_DEMAND_FORECASTING:
		STEP-1: RETRIEVE_SALES_DATA FOR_LAST_180_DAYS (daily_sales_by_product)
		STEP-2: APPLY_TIME_SERIES_FORECASTING MODEL (Prophet_or_ARIMA):
			STEP-2A: LOAD historical_sales_time_series
			STEP-2B: DECOMPOSE: trend + seasonality + residuals
			STEP-2C: INCORPORATE seasonality_factors (weekday_multiplier, holiday_effect, seasonal_patterns)
		STEP-3: INCORPORATE_MARKETING_EVENTS:
			STEP-3A: LOAD upcoming_sales_campaigns, promotions
			STEP-3B: APPLY marketing_boost_multiplier (e.g., 50% sales_lift during_black_friday)
		STEP-4: GENERATE_90_DAY_FORECAST with_confidence_intervals:
			- point_forecast: expected_daily_demand
			- lower_bound: 5th_percentile
			- upper_bound: 95th_percentile
		STEP-5: IDENTIFY_FAST_MOVING_ITEMS (velocity > 50_units_per_week)
		STEP-6: IDENTIFY_SLOW_MOVING_ITEMS (velocity < 5_units_per_week)
		STEP-7: CALCULATE_OPTIMAL_STOCK_LEVEL:
			optimal_stock = (forecast_daily_demand × lead_time_days) + (safety_stock_multiplier × std_deviation)
			WHERE safety_stock_multiplier = 2.33 (95%_service_level) or_3.09 (99%_service_level)

	WORKFLOW_AUTOMATIC_REORDER:
		STEP-1: FOR_PRODUCTS_WITH quantity_available < reorder_level:
		STEP-2: QUERY SUPPLIER_DATABASE: {supplier_id, supplier_price, lead_time_days, min_order_qty}
		STEP-3: CALCULATE_REORDER_QUANTITY:
			reorder_qty = (forecast_demand_90_days × 1.2) - current_on_hand
			(1.2_multiplier provides_20%_safety_buffer)
		STEP-4: VALIDATE reorder_qty >= minimum_order_quantity (supplier_constraint)
		STEP-5: CALCULATE_TOTAL_COST = reorder_qty × supplier_price
		STEP-6: CHECK warehouse_has_sufficient_space:
			IF available_space < (reorder_qty × product_volume): 
				RECOMMEND inventory_optimization (discontinue_slow_moving_items)
		STEP-7: IF_APPROVAL_GRANTED:
			STEP-7A: CREATE_PURCHASE_ORDER IN_PROCUREMENT_SYSTEM
			STEP-7B: UPDATE INVENTORY: quantity_on_order += reorder_qty
			STEP-7C: CALCULATE expected_arrival = today + lead_time_days
			STEP-7D: SCHEDULE_DELIVERY notification TO_WAREHOUSE
			STEP-7E: LOG REORDER_EVENT FOR_ANALYTICS
		STEP-8: IF_APPROVAL_DENIED:
			STEP-8A: MARK product_as_critical_low_stock
			STEP-8B: SEND_ESCALATION_ALERT_TO_PROCUREMENT_TEAM

	WORKFLOW_INVENTORY_OPTIMIZATION:
		STEP-1: IDENTIFY_OVERSTOCKED_ITEMS:
			WHERE quantity_on_hand > (forecast_90_days_demand × 2.0)
		STEP-2: FOR_EACH_OVERSTOCK_ITEM:
			STEP-2A: CALCULATE carrying_cost = (qty_on_hand × warehouse_cost_per_unit_per_day × days_in_stock)
			STEP-2B: CALCULATE obsolescence_risk = f(product_age, last_sold_date, return_rate)
			STEP-2C: IF carrying_cost > profit_on_sale: RECOMMEND_ACTION
		STEP-3: RECOMMEND_ACTIONS FOR_OVERSTOCK:
			OPTION_A: promotional_discount (increase_sales_velocity)
			OPTION_B: bundle_with_fast_moving_items (cross_sell)
			OPTION_C: clearance_sale (aggressive_discount)
			OPTION_D: donate_to_charity (tax_deductible)
		STEP-4: PRIORITIZE_ACTIONS BY:
			(carrying_cost_per_day) × (obsolescence_risk_score) / (expected_revenue_from_action)
		STEP-5: EXECUTE_TOP_RECOMMENDED_ACTION
		STEP-6: TRACK_OUTCOME: measure_sales_improvement_or_reduction_in_inventory

	WORKFLOW_DEAD_STOCK_MANAGEMENT:
		STEP-1: DAILY: IDENTIFY products WITH:
			- zero_sales_in_last_180_days
			- quantity_on_hand > 10_units
			- age_in_inventory > 365_days
		STEP-2: FOR_EACH_DEAD_STOCK_ITEM:
			STEP-2A: CALCULATE total_carrying_cost_incurred
			STEP-2B: CLASSIFY: {low_recovery, medium_recovery, high_recovery}
			STEP-2C: ASSIGN_RECOVERY_ACTION
		STEP-3: RECOVERY_ACTIONS:
			HIGH_RECOVERY: aggressive_discount, bundles, marketing_campaign
			MEDIUM_RECOVERY: moderate_discount, liquidation_to_reseller
			LOW_RECOVERY: donate, recycle, or_write_off
"

MANAGE:"
	REAL_TIME_SYNC: keep_redis_cache in_sync with_primary_inventory_table (5_sec_refresh)
	INVENTORY_TURNOVER_METRICS: track_by_product_and_category, alert_if_decreasing
	STOCKOUT_PREVENTION: alert_if_any_product_out_of_stock > 7_days
	DEAD_STOCK_TRACKING: weekly_identification_and_action_recommendations
	FORECAST_ACCURACY: weekly_comparison forecast_vs_actual_sales, target >= 85%
	WAREHOUSE_UTILIZATION: track_%_of_capacity_used, target_70-85%
"

MAINTAIN:"
	CYCLE_COUNTING: monthly_physical_inventory_verification (rotating_25%_of_SKUs_each_week)
	VARIANCE_TRACKING: reconcile_system_inventory_vs_actual_count, investigate > 5_units
	DATA_QUALITY: ensure_product_id, warehouse_location, reorder_level NOT_NULL
	SUPPLIER_PERFORMANCE: maintain_score(on_time_delivery, quality, price_competitiveness)
	LEAD_TIME_ACCURACY: update_supplier_lead_time_quarterly based_on_recent_orders
"

PROTECT:"
	RULE-1: REQUIRE_APPROVAL for_manual_inventory_adjustments > 50_units
	RULE-2: TRACK_WHO made_each_inventory_change (audit_trail + timestamp)
	RULE-3: IMPLEMENT two_person_verification for_high_value_items (> $10,000)
	RULE-4: PREVENT_NEGATIVE_INVENTORY: if_sale > available_qty, block_transaction
	RULE-5: ALERT_SECURITY if_inventory_shrinkage > 2%_per_month (theft_detection)
"

CREATE:"
	INVENTORY_RECORD WITH FIELDS: {product_id, warehouse_id, qty_on_hand, qty_reserved, qty_available, reorder_level, stock_status, updated_at}
	REORDER_LOG WITH FIELDS: {reorder_id, product_id, reorder_qty, supplier_id, created_at, expected_arrival, status}
"

</agent5>

// ==================== AGENT 6: CUSTOMER SUPPORT INTELLIGENCE AGENT ====================

<agent6>
agent_name="Customer Support Intelligence Agent"
agent_id="AGENT_006_SUPPORT"
priority="HIGH"
instance_count=6
data_access=["USER_DATABASE", "ORDER_DATABASE", "SUPPORT_TICKET_SYSTEM", "FAQ_KNOWLEDGE_BASE", "NLP_MODELS"]
timeout_ms=5000

INTENT:"I WANT TO DELIVER INTELLIGENT, CONTEXT_AWARE CUSTOMER SUPPORT THROUGH SEMANTIC_SEARCH FAQ MATCHING, REAL_TIME ORDER_TRACKING, COMPLAINT_RESOLUTION, AND SEAMLESS ESCALATION TO HUMAN_AGENTS WHILE CONTINUOUSLY IMPROVING RESOLUTION_RATES"

<DATA>
DATA_NAME="CUSTOMER_SUPPORT_PLATFORM"
DATA.PRIVATE()
ENVIRONMENT="@SUPPORT_PAGE,@CHATBOT_INTERFACE,@EMAIL_SUPPORT"{
	START

	SUPPORT_CHANNELS={
		live_chat: "real_time_bot_with_escalation",
		email: "support@ecommerce.local",
		phone: "+1-800-ECOMMERCE",
		social_media: "twitter, facebook_messages",
		knowledge_base: "self_service_faq_and_articles"
	}

	TICKET_CATEGORIZATION={
		categories: [
			"order_tracking",
			"product_quality",
			"delivery_issue",
			"payment_problem",
			"return_request",
			"account_issue",
			"general_inquiry",
			"complaint"
		],
		priority_levels: ["critical", "high", "medium", "low"]
	}

	}
END

CONTEXT={
	LOAD.USER_PROFILE(purchase_history, account_status),
	LOAD.RECENT_ORDERS(status, payment, tracking),
	LOAD.PREVIOUS_SUPPORT_TICKETS(patterns, resolution_rates),
	LOAD.FAQ_DATABASE(embeddings, similarity_scores)
}
</DATA>

EXECUTE:"
	WORKFLOW_CHATBOT_CLASSIFICATION:
		STEP-1: RECEIVE customer_message FROM_CHATBOT_INTERFACE
		STEP-2: CLEAN_INPUT: remove_html, normalize_whitespace, lowercase_for_processing
		STEP-3: PERFORM_SENTIMENT_ANALYSIS USING_VADER_or_BERT:
			- positive_sentiment: score > 0.5
			- neutral_sentiment: -0.1 < score < 0.5
			- negative_sentiment: score < -0.1
		STEP-4: TOKENIZE_MESSAGE AND_APPLY_INTENT_CLASSIFICATION:
			STEP-4A: USE BERT_BASED_INTENT_CLASSIFIER (trained_on_support_tickets)
			STEP-4B: CLASSIFY_INTO ONE_OF predefined_intents
		STEP-5: EXTRACT_ENTITIES USING_NER (Named_Entity_Recognition):
			- product_name
			- order_id
			- issue_category
			- urgency_keywords (urgent, asap, immediately)
		STEP-6: CLASSIFY_ISSUE_CATEGORY using_extracted_entities
		STEP-7: ASSIGN_PRIORITY based_on:
			IF sentiment = 'negative' OR contains_urgency_keywords: priority = 'HIGH'
			ELSE: priority = 'NORMAL'

	WORKFLOW_AUTO_RESPONSE_GENERATION:
		STEP-1: SEARCH_FAQ_DATABASE using_semantic_similarity (embedding_vectors):
			STEP-1A: CONVERT_QUERY_TO_EMBEDDING using_sentence_transformer
			STEP-1B: QUERY ELASTICSEARCH or_pinecone WITH_embedding_vector
			STEP-1C: RETRIEVE TOP_5_MATCHING_FAQ_ARTICLES (cosine_similarity)
		STEP-2: IF best_match_similarity > 0.85:
			STEP-2A: PROVIDE_AUTO_RESPONSE from_FAQ_article
			STEP-2B: APPEND confidence_score and_suggestion_to_improve
		STEP-3: IF CATEGORY IN {order_tracking, payment_status}:
			STEP-3A: QUERY ORDER_TABLE TO GET order_status, tracking_number, delivery_date
			STEP-3B: PROVIDE REAL_TIME_STATUS WITH tracking_link_or_QR_code
			STEP-3C: IF_DELAYED: offer_discount_code or_compensation
		STEP-4: IF CATEGORY = 'return_request':
			STEP-4A: CHECK IF_PRODUCT_ELIGIBLE (within_30_day_return_window)
			STEP-4B: VERIFY return_reason is_valid (damaged, defective, wrong_item)
			STEP-4C: AUTO_APPROVE IF: product_qualifies AND within_window AND valid_reason
			STEP-4D: PROVIDE return_authorization_number + prepaid_shipping_label
		STEP-5: IF confidence_score > 0.9 AND sentiment = 'positive':
			STEP-5A: CLOSE_TICKET automatically as_auto_resolved
		STEP-6: IF confidence_score < 0.7 OR sentiment = 'negative' OR complex_issue:
			STEP-6A: ESCALATE TO human_agent

	WORKFLOW_ESCALATION_TO_HUMAN_AGENT:
		STEP-1: CREATE_SUPPORT_TICKET IN_TICKET_SYSTEM
		STEP-2: SET priority_level based_on_sentiment_and_complexity
		STEP-3: EXTRACT_CONTEXT FOR_AGENT:
			- user_name, email, phone
			- account_status, loyalty_tier
			- order_history, previous_issues
			- chat_transcript, original_question
		STEP-4: DETERMINE_APPROPRIATE_AGENT BASED_ON:
			- AGENT_EXPERTISE: if_technical_issue, assign_tech_support_team
			- AGENT_AVAILABILITY: choose_agent_with_lowest_current_ticket_count
			- LANGUAGE_CAPABILITY: if_customer_language != english, assign_multilingual_agent
			- SPECIALIST_SKILLS: if_vip_customer, assign_senior_agent
		STEP-5: ASSIGN_TICKET TO_SELECTED_AGENT
		STEP-6: SEND_NOTIFICATION_TO_AGENT with_full_context
		STEP-7: SEND_CUSTOMER confirmation_message:
			- ticket_number
			- assigned_agent_name
			- expected_response_time (target: 2_hours)
		STEP-8: SET_SLA_TIMERS:
			- response_sla: 2_hours
			- resolution_sla: 24_hours (normal), 4_hours (high_priority)

	WORKFLOW_COMPLAINT_HANDLING:
		STEP-1: IF customer_complaint detected:
		STEP-2: EXTRACT_COMPLAINT_DETAILS: reason, severity, requested_resolution
		STEP-3: IF REASON = 'product_damaged':
			STEP-3A: REQUEST_PHOTO_EVIDENCE from_customer
			STEP-3B: IF_photo_shows_damage: AUTO_APPROVE refund or_replacement
			STEP-3C: SEND return_label AND_replacement_tracking
		STEP-4: IF REASON = 'delivery_delay' AND days_late > 7:
			STEP-4A: AUTO_ISSUE_CREDIT = shipping_cost × 0.5
			STEP-4B: OFFER_DISCOUNT_COUPON = 10% for_next_purchase
		STEP-5: IF REASON = 'quality_issue':
			STEP-5A: ASSESS_ISSUE_SEVERITY (cosmetic vs_functional)
			STEP-5B: IF_FUNCTIONAL: auto_approve_replacement
			STEP-5C: IF_COSMETIC: offer_discount_or_partial_refund
		STEP-6: IF customer_sentiment = 'very_negative' (VADER_score < -0.8):
			STEP-6A: ESCALATE_IMMEDIATELY TO_SUPERVISOR
			STEP-6B: OFFER_ESCALATED_RESOLUTION (full_refund + discount, loyalty_points)
		STEP-7: LOG_COMPLAINT FOR_ANALYTICS AND_PRODUCT_TEAM_REVIEW

	WORKFLOW_SATISFACTION_MEASUREMENT:
		STEP-1: TICKET_RESOLVED: send_post_resolution_survey
		STEP-2: SURVEY_QUESTIONS:
			- "How_satisfied_were_you_with_resolution?" (1-5_scale)
			- "Would_you_recommend_us?" (yes/no)
			- "What_could_we_improve?" (free_text)
		STEP-3: COLLECT_RESPONSES within_24_hours_of_resolution
		STEP-4: CALCULATE_CSAT_SCORE = (satisfied_responses / total_responses) × 100
		STEP-5: IF_CSAT < 3: automatically_escalate_for_followup
		STEP-6: AGGREGATE_FEEDBACK FOR_CONTINUOUS_IMPROVEMENT
"

MANAGE:"
	TICKET_TRACKING: monitor_ticket_lifecycle (open -> in_progress -> resolved -> closed)
	SLA_COMPLIANCE: ensure_response <= 2_hours AND_resolution <= 24_hours for_high_priority
	FAQ_IMPROVEMENT: weekly_analysis of_low_scoring_faq_matches, update_knowledge_base
	CHATBOT_ACCURACY: track_auto_resolution_success_rate (target > 70%)
	CUSTOMER_SATISFACTION: send_post_resolution_surveys, target_csat >= 4.5/5.0
	AGENT_PERFORMANCE: track_avg_resolution_time, csat_score, escalation_rate
"

MAINTAIN:"
	KNOWLEDGE_BASE: update_faq_monthly with_new_issues_and_resolutions
	MODEL_RETRAINING: retrain_intent_classifier monthly_with_new_conversation_data
	CHAT_HISTORY: maintain_full_chat_history in_database (for_compliance_and_training)
	ESCALATION_TRACKING: ensure_no_ticket escalates > 2_times (address_root_causes)
	DATA_QUALITY: validate_ticket_description NOT_NULL, sentiment_score IN_[-1,1]
"

PROTECT:"
	RULE-1: REQUIRE_VERIFICATION (order_id + email) before_revealing_order_details
	RULE-2: LIMIT_RESOLUTION_AUTHORITY: auto_refund_cap = $500 (above_requires_supervisor)
	RULE-3: TRACK_REFUND_FRAUD: flag_users with > 3_refunds_in_30_days
	RULE-4: ENCRYPT_ALL_CHAT_HISTORY in_database (AES_256)
	RULE-5: RATE_LIMIT_CHATBOT: max_50_messages_per_hour_per_user (prevent_abuse)
"

CREATE:"
	SUPPORT_TICKET WITH FIELDS: {ticket_id, user_id, category, description, sentiment, priority, created_at, status, assigned_agent}
	CHAT_MESSAGE LOG WITH FIELDS: {message_id, ticket_id, sender_type, timestamp, content, sentiment_score}
"

</agent6>

// ==================== AGENT 7: REAL_TIME ANALYTICS AND MONITORING AGENT ====================

<agent7>
agent_name="Real-Time Analytics and Monitoring Agent"
agent_id="AGENT_007_ANALYTICS"
priority="CRITICAL"
instance_count=4
data_access=["ANALYTICS_DATABASE", "ELASTICSEARCH", "USER_BEHAVIOR_LOGS", "ORDER_DATABASE", "SYSTEM_METRICS"]
timeout_ms=2000

INTENT:"I WANT TO CONTINUOUSLY MONITOR SYSTEM_HEALTH, TRACK KEY_BUSINESS_METRICS IN REAL_TIME, GENERATE_ACTIONABLE_INSIGHTS, DETECT_ANOMALIES, AND PROVIDE_ALERTS TO ENABLE DATA_DRIVEN_DECISION_MAKING"

<DATA>
DATA_NAME="ANALYTICS_AND_MONITORING_SYSTEM"
DATA.SCOPE="PUBLIC_AGGREGATE_ONLY"
ENVIRONMENT="@ANALYTICS_DASHBOARD,@ALERTS_SYSTEM,@EXECUTIVE_DASHBOARD"{
	START

	METRICS_CATEGORIES={
		business_metrics: [
			daily_revenue,
			daily_orders,
			avg_order_value,
			conversion_rate,
			cart_abandonment_rate,
			repeat_purchase_rate,
			customer_lifetime_value
		],
		user_metrics: [
			daily_active_users,
			new_signups,
			user_retention_rate,
			churn_rate,
			engagement_score
		],
		product_metrics: [
			top_selling_products,
			low_selling_products,
			inventory_turnover,
			stockout_rate,
			return_rate
		],
		system_metrics: [
			api_response_time,
			error_rate,
			uptime_percentage,
			cache_hit_ratio,
			database_query_time,
			cpu_utilization,
			memory_utilization
		],
		customer_metrics: [
			satisfaction_score,
			support_response_time,
			refund_rate,
			complaint_rate,
			net_promoter_score
		]
	}

	}
END

CONTEXT={
	LOAD.REALTIME_METRICS(from_application_instrumentation),
	LOAD.HISTORICAL_METRICS(from_analytics_database),
	LOAD.SYSTEM_EVENTS(logs, alerts, deployments),
	LOAD.BASELINE_EXPECTATIONS(7_day_moving_average)
}
</DATA>

EXECUTE:"
	WORKFLOW_METRICS_COLLECTION:
		STEP-1: EVERY_1_MINUTE: collect_system_metrics
			- api.request.count
			- api.request.latency (p50, p95, p99)
			- cache.hit.ratio
			- cache.miss.ratio
			- error.rate (by_endpoint)
			- memory.usage (%)
			- cpu.usage (%)
			- disk.usage (%)
			- active.requests
			- queue.length
			- db.connection.pool.available

		STEP-2: EVERY_5_MINUTES: collect_application_metrics
			- daily.active.users (DAU)
			- current.concurrent.users
			- api.calls.by_endpoint (top_10)
			- api.errors.by_endpoint
			- database.query.count
			- database.slow.queries (> 100ms)
			- session.count

		STEP-3: EVERY_1_HOUR: collect_business_metrics
			- total.revenue (hourly)
			- orders.count
			- avg.order.value
			- conversion.rate (hourly)
			- new.signups (hourly)
			- top.products (by_sales)
			- inventory.status (low_stock_count)

		STEP-4: EVERY_1_DAY: collect_aggregate_metrics
			- daily.revenue
			- daily.orders
			- daily.new.users
			- user.retention.rate
			- churn.rate
			- repeat.purchase.rate

		STEP-5: AGGREGATE all_metrics AND_store IN time_series_database (InfluxDB_or_Prometheus)

	WORKFLOW_ANOMALY_DETECTION:
		STEP-1: FOR_EACH_METRIC_COLLECTED:
		STEP-2: CALCULATE_7_DAY_MOVING_AVERAGE (baseline_expectation)
		STEP-3: CALCULATE_STANDARD_DEVIATION around_moving_average
		STEP-4: SET_ANOMALY_THRESHOLD = baseline_average ± (2 × std_deviation)
		STEP-5: IF current_metric > anomaly_threshold:
			STEP-5A: LOG_ANOMALY with_severity_level (info, warning, critical)
			STEP-5B: IF severity = 'critical': trigger_immediate_alert
			STEP-5C: IF severity = 'warning': log_for_review
		STEP-6: APPLY_CONTEXT_FILTERS:
			- account_for_weekday_variations (weekends_have_lower_traffic)
			- account_for_seasonal_patterns (holiday_traffic_spikes)
			- account_for_deployment_windows (don't_alert_during_maintenance)
		STEP-7: IDENTIFY_SPECIFIC_ANOMALIES:
			IF error.rate > 5%: alert("High_error_rate_detected")
			IF api.latency.p95 > 500ms: alert("Slow_API_response_detected")
			IF revenue < baseline × 0.7: alert("Revenue_drop_detected")
			IF user.churn > 3%_in_week: alert("Abnormal_churn_rate")
			IF cache.hit.ratio < 50%: alert("Cache_performance_degradation")
			IF database.slow.queries > 100: alert("Database_performance_issue")
			IF uptime < 99.5%: alert("Uptime_SLA_at_risk")

	WORKFLOW_DASHBOARD_GENERATION:
		STEP-1: GENERATE_REAL_TIME_DASHBOARD (refresh_every_30_seconds):
			- current_concurrent_users
			- current_error_rate
			- current_api_latency_p95
			- current_cache_hit_ratio
			- today's_revenue_so_far
			- today's_order_count

		STEP-2: GENERATE_DAILY_REPORT (at_11:59pm):
			- daily.revenue
			- daily.orders
			- daily.users
			- conversion.rate
			- customer.satisfaction
			- top_10_products

		STEP-3: GENERATE_WEEKLY_REPORT (every_monday):
			- week_over_week_comparison (revenue, orders, users)
			- sales_trends
			- traffic_trends
			- product_performance (top_gainers, top_losers)
			- customer_acquisition_metrics

		STEP-4: GENERATE_MONTHLY_REPORT (every_1st_of_month):
			- month_over_month_comparison
			- revenue_breakdown (by_category, by_region)
			- customer_insights (new_users, retention, ltv)
			- seasonal_patterns
			- key_achievements_and_challenges

		STEP-5: CREATE_EXECUTIVE_DASHBOARD FOR_C_LEVEL:
			- GMV (Gross_Merchandise_Value)
			- growth_rate (month_over_month)
			- profitability (net_revenue - costs)
			- user_acquisition_cost (UAC)
			- customer_lifetime_value (LTV)
			- ltv_uac_ratio (should_be > 3:1)

		STEP-6: CREATE_OPERATIONAL_DASHBOARD FOR_TEAMS:
			- queue_length (by_service)
			- error_rates (by_endpoint, by_service)
			- sla_compliance (response_time, resolution_time)
			- agent_productivity (support_team)
			- inventory_health (low_stock_alerts, overstock_items)
"

MANAGE:"
	ALERT_ROUTING: send_critical_alerts_to on_call_engineer, high_alerts_to_team_lead, normal_to_slack
	ALERT_SUPPRESSION: prevent_duplicate_alerts within_5_minutes (reduce_noise)
	METRIC_RETENTION: keep_detailed_metrics_90_days, then_downsample_to_hourly_for_2_years
	DASHBOARD_AUTO_REFRESH: update_real_time_dashboards_every_30_seconds
	SLA_TRACKING: monitor_uptime_percentage (target_99.9%), alert_if < 99.5%
	FORECAST_ACCURACY: track_prediction_accuracy_of_demand_forecasts
"

MAINTAIN:"
	DATA_QUALITY: validate_metrics NOT_NULL, timestamp_valid, value_numeric
	METRIC_CONSISTENCY: reconcile_daily_revenue_from_orders against_daily_transactions_total
	INSTRUMENTATION_CHECKS: weekly_verification all_services_report_metrics
	ALERT_ACCURACY: quarterly_review false_positives_and_tune_anomaly_thresholds
	BASELINE_UPDATES: update_baseline_expectations_weekly based_on_latest_trends
"

PROTECT:"
	RULE-1: AGGREGATE_SENSITIVE_METRICS (never_expose_individual_user_metrics_or_pii)
	RULE-2: IMPLEMENT_ACCESS_CONTROL (only_authorized_team_members_can_view_dashboards)
	RULE-3: RATE_LIMIT_ANALYTICS_API (max_100_requests_per_minute_per_user)
	RULE-4: IMPLEMENT_QUERY_TIMEOUT (analytics_queries_max_5_minutes_execution)
	RULE-5: ENCRYPT_DASHBOARD_ACCESS (https_only, strong_authentication)
"

CREATE:"
	METRICS_RECORD WITH FIELDS: {metric_name, value, timestamp, dimension(region/product/user_segment), tags, aggregation_level}
	ALERT_EVENT WITH FIELDS: {alert_id, metric_name, threshold_violated, actual_value, severity, created_at, acknowledged_at, acknowledged_by}
"

</agent7>

// ==================== MULTI_AGENT ORCHESTRATION AND COORDINATION ====================

<agentic>
agentic_name="E-Commerce Agentic AI Master Orchestrator"
agentic_id="AGENTIC_MASTER"
priority="CRITICAL"
deployment_model="distributed_event_driven"

INTENT:"I WANT TO ORCHESTRATE ALL_7_AGENTS (USER_AUTH, PRODUCT_SEARCH, RECOMMENDATIONS, CART_CHECKOUT, INVENTORY, SUPPORT, ANALYTICS) IN A COORDINATED, INTELLIGENT MANNER TO DELIVER A SEAMLESS, PERSONALIZED, AND PROFITABLE E_COMMERCE EXPERIENCE"

DECLARE{
	COORDINATION_LEVEL: system_wide,
	COMMUNICATION_PATTERN: event_driven_publish_subscribe,
	FAULT_TOLERANCE: circuit_breaker_with_fallbacks,
	DATA_CONSISTENCY: eventual_consistency_with_distributed_transactions
}

ACTION:"
	// ==================== AGENT INTERACTION SEQUENCES ====================

	SEQUENCE_1_USER_BROWSING_EXPERIENCE:
		TRIGGER: user_lands_on_homepage
		AGENTS_INVOLVED: [USER_AUTH, PRODUCT_SEARCH, RECOMMENDATIONS, ANALYTICS]
		
		CHECKPOINT_1_AUTH: USER_AUTH_AGENT
			- verify_user_session or_require_login
			- load_user_profile_and_preferences
			- setup_session_tracking
		
		CHECKPOINT_2_ANALYTICS: ANALYTICS_AGENT
			- log_user_session_start
			- initialize_behavior_tracking
			- record_page_view_event
		
		CHECKPOINT_3_SEARCH: PRODUCT_DISCOVERY_AGENT
			- load_trending_products
			- load_featured_products
			- prepare_search_interface
		
		CHECKPOINT_4_RECOMMENDATIONS: RECOMMENDATION_ENGINE_AGENT
			- generate_personalized_homepage_recommendations
			- calculate_relevance_scores
			- rank_products_for_display
		
		CHECKPOINT_5_ANALYTICS: ANALYTICS_AGENT
			- track_recommendations_impressions
			- record_product_views
			- monitor_engagement_metrics
		
		EXECUTION_FLOW: USER_AUTH -> ANALYTICS -> PRODUCT_SEARCH -> RECOMMENDATIONS -> ANALYTICS
		SUCCESS_CRITERIA: [homepage_load_time < 2s, recommendations_generated_within_1s]

	SEQUENCE_2_PRODUCT_SEARCH_AND_DISCOVERY:
		TRIGGER: user_enters_search_query or_browses_category
		AGENTS_INVOLVED: [PRODUCT_SEARCH, RECOMMENDATIONS, INVENTORY, ANALYTICS]
		
		CHECKPOINT_1_SEARCH: PRODUCT_DISCOVERY_AGENT
			- receive_search_query
			- execute_elasticsearch_search
			- apply_filters_and_sorting
			- fetch_real_time_inventory_status
		
		CHECKPOINT_2_INVENTORY: INVENTORY_MANAGEMENT_AGENT
			- verify_product_availability
			- update_stock_status_display
			- flag_low_stock_items
		
		CHECKPOINT_3_RECOMMENDATIONS: RECOMMENDATION_ENGINE_AGENT
			- generate_related_products
			- cross_sell_suggestions
			- frequently_bought_together
		
		CHECKPOINT_4_ANALYTICS: ANALYTICS_AGENT
			- log_search_query_event
			- track_filter_usage
			- record_result_clicks
		
		EXECUTION_FLOW: PRODUCT_SEARCH -> INVENTORY -> RECOMMENDATIONS -> ANALYTICS
		SUCCESS_CRITERIA: [search_latency < 500ms, result_accuracy >= 90%]

	SEQUENCE_3_ADD_TO_CART_AND_CHECKOUT:
		TRIGGER: user_clicks_add_to_cart
		AGENTS_INVOLVED: [INVENTORY, CART_CHECKOUT, ANALYTICS, RECOMMENDATIONS]
		
		CHECKPOINT_1_INVENTORY: INVENTORY_MANAGEMENT_AGENT
			- verify_product_still_available
			- reserve_inventory_for_10_minutes
			- update_available_quantity_display
		
		CHECKPOINT_2_CART: SHOPPING_CART_AGENT
			- add_item_to_cart
			- calculate_line_total
			- apply_any_automatic_discounts
			- update_cart_subtotal
		
		CHECKPOINT_3_RECOMMENDATIONS: RECOMMENDATION_ENGINE_AGENT
			- generate_next_best_recommendations
			- suggest_complementary_products
			- show_frequently_bought_together
		
		CHECKPOINT_4_ANALYTICS: ANALYTICS_AGENT
			- log_add_to_cart_event
			- track_funnel_progression
			- monitor_cart_value_trends
		
		EXECUTION_FLOW: INVENTORY -> CART -> RECOMMENDATIONS -> ANALYTICS
		SUCCESS_CRITERIA: [cart_update_latency < 500ms, inventory_consistency 100%]

	SEQUENCE_4_PAYMENT_AND_ORDER_COMPLETION:
		TRIGGER: user_clicks_place_order
		AGENTS_INVOLVED: [CART_CHECKOUT, INVENTORY, ANALYTICS, SUPPORT]
		
		CHECKPOINT_1_CHECKOUT: SHOPPING_CART_AGENT
			- validate_cart_items
			- calculate_final_price (subtotal + tax + shipping - discount)
			- prepare_payment_information
		
		CHECKPOINT_2_PAYMENT: SHOPPING_CART_AGENT (payment_processing)
			- process_payment_via_stripe
			- implement_3d_secure_if_needed
			- handle_payment_decline_with_retries
		
		CHECKPOINT_3_INVENTORY: INVENTORY_MANAGEMENT_AGENT (on_payment_success)
			- decrement_inventory_quantities
			- finalize_inventory_reservation
			- trigger_reorder_if_stock_low
		
		CHECKPOINT_4_SUPPORT: CUSTOMER_SUPPORT_AGENT
			- send_order_confirmation_email
			- generate_tracking_number
			- prepare_faq_for_order_tracking
		
		CHECKPOINT_5_ANALYTICS: ANALYTICS_AGENT
			- log_order_placed_event
			- update_revenue_metrics
			- track_conversion_funnel_completion
			- record_customer_acquisition_cost
		
		EXECUTION_FLOW: CART -> INVENTORY -> SUPPORT -> ANALYTICS
		SUCCESS_CRITERIA: [payment_latency < 5s, payment_success_rate > 98%]

	SEQUENCE_5_POST_PURCHASE_ENGAGEMENT:
		TRIGGER: order_successfully_placed
		AGENTS_INVOLVED: [INVENTORY, RECOMMENDATIONS, SUPPORT, ANALYTICS]
		
		CHECKPOINT_1_INVENTORY: INVENTORY_MANAGEMENT_AGENT
			- monitor_inventory_levels
			- trigger_reorder_if_stock_critical
			- identify_fast_moving_items
		
		CHECKPOINT_2_RECOMMENDATIONS: RECOMMENDATION_ENGINE_AGENT
			- generate_post_purchase_recommendations
			- suggest_complementary_products
			- prepare_email_campaign_content
		
		CHECKPOINT_3_SUPPORT: CUSTOMER_SUPPORT_AGENT
			- send_order_updates (processing, shipped, delivered)
			- prepare_return_instructions
			- setup_post_delivery_survey
		
		CHECKPOINT_4_ANALYTICS: ANALYTICS_AGENT
			- calculate_customer_lifetime_value
			- update_repeat_purchase_probability
			- track_product_affinity_scores
		
		EXECUTION_FLOW: INVENTORY -> RECOMMENDATIONS -> SUPPORT -> ANALYTICS
		SUCCESS_CRITERIA: [repeat_purchase_rate > 30%, customer_satisfaction >= 4.5/5]

	SEQUENCE_6_CUSTOMER_SUPPORT_INCIDENT:
		TRIGGER: customer_creates_support_ticket
		AGENTS_INVOLVED: [SUPPORT, USER_AUTH, INVENTORY, ANALYTICS]
		
		CHECKPOINT_1_SUPPORT: CUSTOMER_SUPPORT_AGENT
			- receive_support_request
			- classify_issue_category_using_nlp
			- analyze_sentiment
		
		CHECKPOINT_2_AUTH: USER_AUTH_AGENT
			- verify_customer_identity
			- load_customer_context (account_history, preferences)
		
		CHECKPOINT_3_SUPPORT: CUSTOMER_SUPPORT_AGENT (auto_resolution_attempt)
			- search_faq_database_using_semantic_similarity
			- attempt_auto_resolution_if_high_confidence
			- escalate_to_human_agent_if_low_confidence
		
		CHECKPOINT_4_ANALYTICS: ANALYTICS_AGENT
			- log_support_ticket_event
			- track_resolution_time
			- monitor_csat_score
		
		EXECUTION_FLOW: SUPPORT -> AUTH -> SUPPORT (attempt_auto) -> ANALYTICS
		SUCCESS_CRITERIA: [auto_resolution_rate > 70%, csat >= 4.5/5]

	SEQUENCE_7_SYSTEM_MONITORING_AND_OPTIMIZATION:
		TRIGGER: continuous_monitoring (every_1_minute)
		AGENTS_INVOLVED: [ANALYTICS, INVENTORY, RECOMMENDATIONS]
		
		CHECKPOINT_1_ANALYTICS: ANALYTICS_AGENT
			- collect_system_metrics
			- detect_anomalies
			- generate_alerts_if_needed
		
		CHECKPOINT_2_INVENTORY: INVENTORY_MANAGEMENT_AGENT
			- monitor_inventory_health
			- identify_low_stock_items
			- generate_reorder_recommendations
		
		CHECKPOINT_3_RECOMMENDATIONS: RECOMMENDATION_ENGINE_AGENT
			- monitor_recommendation_quality_metrics
			- detect_model_drift
			- schedule_retraining_if_needed
		
		EXECUTION_FLOW: ANALYTICS -> INVENTORY -> RECOMMENDATIONS
		SUCCESS_CRITERIA: [uptime > 99.9%, all_metrics_within_baseline]

INTER_AGENT_COMMUNICATION_PROTOCOL:
	- communication_method: event_bus (publish_subscribe_pattern)
	- message_queue: rabbitmq_or_kafka
	- message_serialization: json
	- event_processing_latency_sla: < 100ms
	- message_retention: 7_days
	- dead_letter_queue: for_failed_messages

DEFINED_EVENTS:
	- user_logged_in
	- user_logged_out
	- search_query_submitted
	- product_viewed
	- item_added_to_cart
	- item_removed_from_cart
	- cart_abandoned
	- checkout_started
	- payment_initiated
	- payment_successful
	- payment_failed
	- order_placed
	- order_confirmed
	- order_shipped
	- order_delivered
	- inventory_low
	- inventory_out_of_stock
	- product_returned
	- support_ticket_created
	- support_ticket_resolved
	- recommendation_generated
	- anomaly_detected
	- retraining_completed

AGENT_HEALTH_AND_FAULT_TOLERANCE:
	- health_check_interval: 30_seconds
	- heartbeat_timeout: 2_minutes
	- circuit_breaker_threshold: 5_consecutive_failures
	- circuit_breaker_timeout: 1_minute
	- fallback_strategies: {
		if_search_fails: return_last_cached_results,
		if_recommendations_fail: return_trending_products,
		if_payment_fails: retry_with_exponential_backoff,
		if_inventory_fails: block_checkout_and_notify_user,
		if_support_fails: escalate_to_human_agent
	}
	- automatic_service_recovery: enabled
	- graceful_degradation: enabled

RULE:"
	COORDINATION_RULES:
		RULE-1: no_conflicting_operations: if_inventory_update_in_progress, block_checkout
		RULE-2: maintain_data_consistency: use_distributed_transactions_with_2_phase_commit
		RULE-3: implement_circuit_breaker: if_agent_fails, reroute_to_fallback
		RULE-4: implement_timeout_handling: if_agent_takes > 30_seconds, timeout_and_provide_fallback
		RULE-5: implement_idempotency: agents_safe_to_retry (idempotency_keys)
		RULE-6: prevent_cascade_failures: implement_bulkheads_and_limits

	DATA_CONSISTENCY_RULES:
		RULE-7: eventual_consistency: accept_minor_delays in_data_propagation
		RULE-8: critical_operations: use_strong_consistency (payments, inventory)
		RULE-9: reconciliation: hourly_audit_of_data_consistency_across_agents
		RULE-10: conflict_resolution: last_write_wins for_non_critical_data, snapshot_isolation for_transactions
"

MAINTAIN:"
	distributed_tracing: log_request_trace_id across_all_agents for_debugging
	performance_monitoring: track_inter_agent_latency, alert_if > 500ms
	message_queue_monitoring: alert_if_queue_depth > 10000 or_latency > 5s
	agent_log_aggregation: centralized_logging_to_elk_stack
	chaos_engineering: weekly_failure_injection_tests to_ensure_fault_tolerance
"

PROTECT:"
	all_inter_agent_communication: encrypted using_tls_1.3
	message_authentication: signed using_hmac_sha256
	rate_limiting: between_agents, max_1000_messages_per_second
	authorization: each_agent_only_accesses_data_it_needs
	audit_logging: all_inter_agent_operations_logged_for_compliance
"

</agentic>

// ==================== API LAYER DEFINITION ====================

<API>
api_name="E-Commerce REST API v1"
base_url="https://api.ecommerce.local/v1"
authentication="Bearer JWT Token"
rate_limiting="1000 requests per minute per user"
response_format="application/json"
error_handling="RFC 7807 Problem+JSON"

VERSION_STRATEGY="semantic_versioning"
DEPRECATED_VERSIONS=[]
CURRENT_VERSION="v1"
NEXT_VERSION="v2_planned"

ENDPOINTS={
	// ==================== AUTHENTICATION ENDPOINTS ====================
	POST /auth/register={
		description: "Register new user account",
		parameters: {email, password, full_name, phone_number},
		response: {user_id, email, status},
		rate_limit: "10 per minute",
		requires_auth: false
	},

	POST /auth/login={
		description: "Authenticate user and return JWT token",
		parameters: {email, password, mfa_code_optional},
		response: {access_token, refresh_token, user},
		rate_limit: "5 per minute",
		requires_auth: false
	},

	POST /auth/logout={
		description: "Invalidate user session",
		parameters: {},
		response: {success: true},
		requires_auth: true
	},

	POST /auth/refresh-token={
		description: "Refresh access token using refresh token",
		parameters: {refresh_token},
		response: {access_token, expires_in},
		rate_limit: "30 per minute",
		requires_auth: false
	},

	POST /auth/forgot-password={
		description: "Request password reset",
		parameters: {email},
		response: {message: "Password reset email sent"},
		rate_limit: "3 per minute per email",
		requires_auth: false
	},

	POST /auth/reset-password={
		description: "Reset password using token",
		parameters: {token, new_password},
		response: {success: true},
		requires_auth: false
	},

	// ==================== USER PROFILE ENDPOINTS ====================
	GET /users/profile={
		description: "Get authenticated user profile",
		response: {user_id, email, full_name, phone, preferences, loyalty_tier},
		requires_auth: true
	},

	PUT /users/profile={
		description: "Update user profile",
		parameters: {full_name, phone_number, bio, preferences},
		response: {updated_user},
		requires_auth: true
	},

	GET /users/addresses={
		description: "Get user's saved addresses",
		response: [{address_id, street, city, state, zip, country, is_default}],
		requires_auth: true
	},

	POST /users/addresses={
		description: "Add new address",
		parameters: {street, city, state, zip, country, address_type},
		response: {address_id},
		requires_auth: true
	},

	// ==================== PRODUCT SEARCH ENDPOINTS ====================
	GET /products/search={
		description: "Search products with filters",
		parameters: {
			q: "search query",
			category_id: "optional",
			min_price: "optional",
			max_price: "optional",
			rating: "optional",
			sort_by: "relevance|price_asc|price_desc|rating|newest",
			page: 1,
			page_size: 20
		},
		response: {
			results: [{product_id, name, price, rating, image_url}],
			total_count: 234,
			facets: {categories, price_ranges, brands}
		},
		rate_limit: "100 per minute",
		requires_auth: false
	},

	GET /products/{product_id}={
		description: "Get detailed product information",
		response: {
			product_id, name, description, price, rating, reviews,
			images, specifications, inventory_status, recommendations
		},
		requires_auth: false
	},

	GET /products/recommendations={
		description: "Get personalized recommendations",
		parameters: {
			context: "homepage|product_page|checkout",
			limit: 20
		},
		response: {recommendations: [{product_id, name, price, reason}]},
		requires_auth: true
	},

	// ==================== SHOPPING CART ENDPOINTS ====================
	GET /cart={
		description: "Get shopping cart",
		response: {
			cart_id, items: [{product_id, name, quantity, price}],
			subtotal, tax, shipping, discount, total
		},
		requires_auth: true
	},

	POST /cart/items={
		description: "Add item to cart",
		parameters: {product_id, quantity},
		response: {cart},
		requires_auth: true
	},

	PUT /cart/items/{item_id}={
		description: "Update cart item quantity",
		parameters: {quantity},
		response: {cart},
		requires_auth: true
	},

	DELETE /cart/items/{item_id}={
		description: "Remove item from cart",
		response: {cart},
		requires_auth: true
	},

	POST /cart/promo-code={
		description: "Apply promo code to cart",
		parameters: {promo_code},
		response: {discount_applied, new_total},
		requires_auth: true
	},

	// ==================== CHECKOUT ENDPOINTS ====================
	POST /checkout/calculate-shipping={
		description: "Calculate shipping cost",
		parameters: {address_id, shipping_method},
		response: {shipping_cost, estimated_delivery_date},
		requires_auth: true
	},

	POST /checkout/calculate-tax={
		description: "Calculate tax based on address",
		parameters: {address_id},
		response: {tax_amount, tax_rate},
		requires_auth: true
	},

	POST /orders={
		description: "Create order (place order)",
		parameters: {
			shipping_address_id,
			billing_address_id,
			shipping_method,
			payment_method
		},
		response: {order_id, order_number, total_amount, status},
		requires_auth: true
	},

	// ==================== PAYMENT ENDPOINTS ====================
	POST /payments/process={
		description: "Process payment",
		parameters: {order_id, payment_token, amount},
		response: {transaction_id, status},
		requires_auth: true,
		rate_limit: "10 per hour per user"
	},

	GET /payments/{payment_id}={
		description: "Get payment status",
		response: {payment_id, order_id, status, amount},
		requires_auth: true
	},

	// ==================== ORDER ENDPOINTS ====================
	GET /orders={
		description: "Get user's orders",
		parameters: {status: "all|pending|shipped|delivered", page: 1},
		response: {orders: [{order_id, order_date, total, status}]},
		requires_auth: true
	},

	GET /orders/{order_id}={
		description: "Get order details",
		response: {
			order_id, order_date, items, shipping_address,
			total, status, tracking_number, estimated_delivery
		},
		requires_auth: true
	},

	GET /orders/{order_id}/tracking={
		description: "Get shipment tracking",
		response: {
			order_id, tracking_number, carrier, status,
			timeline: [{status, timestamp, location}]
		},
		requires_auth: true
	},

	POST /orders/{order_id}/cancel={
		description: "Cancel order",
		response: {success: true, refund_status},
		requires_auth: true
	},

	POST /orders/{order_id}/return={
		description: "Initiate return",
		parameters: {reason, items: [{product_id, quantity}]},
		response: {return_id, authorization_number, shipping_label},
		requires_auth: true
	},

	// ==================== SUPPORT ENDPOINTS ====================
	GET /support/faq={
		description: "Search FAQ articles",
		parameters: {q: "search query", page: 1},
		response: {articles: [{article_id, title, content}]},
		requires_auth: false
	},

	POST /support/tickets={
		description: "Create support ticket",
		parameters: {category, subject, description, order_id_optional},
		response: {ticket_id, ticket_number, status},
		requires_auth: true
	},

	GET /support/tickets/{ticket_id}={
		description: "Get support ticket details",
		response: {
			ticket_id, category, subject, status,
			messages: [{sender, timestamp, content}]
		},
		requires_auth: true
	},

	POST /support/tickets/{ticket_id}/messages={
		description: "Add message to support ticket",
		parameters: {message_content},
		response: {message_id, timestamp},
		requires_auth: true
	},

	// ==================== ANALYTICS ENDPOINTS (INTERNAL) ====================
	GET /analytics/dashboard={
		description: "Get analytics dashboard data",
		response: {
			daily_revenue, daily_orders, avg_order_value,
			conversion_rate, user_count, top_products
		},
		requires_auth: true,
		role_required: "admin_or_analytics"
	},

	GET /analytics/reports/{report_type}={
		description: "Get detailed analytics report",
		parameters: {date_from, date_to, dimension_optional},
		response: {report_data},
		requires_auth: true,
		role_required: "admin_or_analytics"
	}
}

ERROR_RESPONSES={
	400_bad_request: "Invalid request parameters",
	401_unauthorized: "Authentication required or token invalid",
	403_forbidden: "Insufficient permissions",
	404_not_found: "Resource not found",
	409_conflict: "Resource already exists or state conflict",
	429_too_many_requests: "Rate limit exceeded",
	500_internal_server_error: "Internal server error",
	503_service_unavailable: "Service temporarily unavailable"
}

</API>

// ==================== FRONTEND TECHNOLOGY STACK ====================

<FRONTEND>
framework="React 18"
state_management="Redux Toolkit"
ui_library="Material-UI 5"
http_client="Axios"
routing="React Router v6"
styling="CSS-in-JS (emotion or styled-components)"
build_tool="Vite"
package_manager="npm or yarn"

PAGES={
	home_page: "landing page with featured products and recommendations",
	search_results: "search results with filters, sorting, pagination",
	product_detail: "full product details with reviews, recommendations",
	shopping_cart: "cart management with checkout button",
	checkout: "multi-step checkout with shipping, tax, payment",
	account_dashboard: "user profile, orders, preferences",
	support_center: "faq, ticket creation, chat interface",
	admin_dashboard: "analytics, inventory, order management"
}

COMPONENTS={
	header: "navigation, search bar, cart icon, user menu",
	product_grid: "grid/list view of products with filters",
	product_card: "individual product display",
	search_bar: "with autocomplete and suggestions",
	shopping_cart_widget: "floating cart summary",
	checkout_form: "address, payment info, order review",
	support_chat: "ai chatbot interface",
	analytics_dashboard: "charts, metrics, kpis"
}

STYLING={
	primary_color: "rgb(0, 150, 190)",
	secondary_color: "rgb(255, 107, 107)",
	background_color: "rgb(250, 250, 250)",
	text_color: "rgb(40, 40, 40)",
	font_family: "'Segoe UI', Tahoma, Geneva, Verdana, sans-serif",
	animation_duration: "300ms",
	responsive_breakpoints: [320, 768, 1024, 1440]
}

ACCESSIBILITY={
	wcag_level: "AA",
	keyboard_navigation: "enabled",
	screen_reader_support: "aria_labels",
	color_contrast: ">= 4.5:1",
	focus_indicators: "visible"
}

</FRONTEND>

// ==================== BACKEND ARCHITECTURE ====================

<BACKEND>
language="Python"
framework="Django 4.2"
api_framework="Django REST Framework 3.14"
database="PostgreSQL 15 (primary + read replicas)"
cache="Redis 7 (distributed cache)"
search_engine="Elasticsearch 8 (product search)"
message_queue="RabbitMQ 3.12 (async tasks)"
task_scheduler="Celery 5.2 (background jobs)"
web_server="Gunicorn (WSGI)"

DEPLOYMENT={
	containerization: "Docker",
	orchestration: "Kubernetes",
	cloud_provider: "AWS",
	regions: ["us-east-1", "eu-west-1", "ap-southeast-1"],
	auto_scaling: true,
	load_balancer: "AWS Application Load Balancer",
	cdn: "CloudFront for static assets",
	storage: "S3 for user uploads and backups"
}

MONITORING_AND_LOGGING={
	monitoring: "Prometheus + Grafana",
	logging: "ELK Stack (Elasticsearch, Logstash, Kibana)",
	apm: "Datadog or New Relic",
	error_tracking: "Sentry"
}

SECURITY={
	tls_version: "1.3",
	cipher_suites: ["TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256"],
	api_key_management: "rotating_keys_every_90_days",
	secret_management: "AWS Secrets Manager",
	rate_limiting: "Redis-based token bucket algorithm",
	cors: "configured for allowed_domains",
	csrf_protection: "django_csrf_middleware",
	sql_injection_prevention: "parameterized_queries_orm",
	xss_prevention: "input_sanitization + csp_headers"
}

SCALABILITY_FEATURES={
	database_sharding: "by_user_id_or_product_id_if_needed",
	read_replicas: "3_replicas_across_regions",
	connection_pooling: "pgbouncer_for_postgresql",
	cache_invalidation: "event_driven_with_ttl",
	async_processing: "celery_for_heavy_operations",
	batch_processing: "daily_jobs_for_analytics_aggregation"
}

</BACKEND>

// ==================== DEPLOYMENT AND INFRASTRUCTURE ====================

<DEPLOYMENT>
strategy="rolling_deployment_with_blue_green_fallback"
frequency="continuous_deployment"
cicd_platform="github_actions_or_gitlab_ci"
artifact_registry="docker_hub_or_ecr"

DEPLOYMENT_PIPELINE={
	stage_1_code: "commit_to_github",
	stage_2_build: "docker_image_build_and_push",
	stage_3_test: "unit_tests_integration_tests",
	stage_4_staging: "deploy_to_staging_environment",
	stage_5_validation: "smoke_tests_and_e2e_tests",
	stage_6_production: "deploy_to_production_with_canary",
	stage_7_monitoring: "health_checks_and_alerts"
}

ROLLBACK_STRATEGY={
	trigger_condition: "error_rate > 5% OR latency_p95 > 1000ms",
	rollback_time: "< 2 minutes",
	automated: true
}

DISASTER_RECOVERY={
	backup_frequency: "hourly_database_backups",
	backup_retention: "30_days",
	rto: "< 1_hour",
	rpo: "< 15_minutes",
	multi_region_failover: true,
	tested_quarterly: true
}

</DEPLOYMENT>

// ==================== SUCCESS METRICS AND KPIS ====================

<METRICS>

BUSINESS_KPIS={
	revenue: {
		target: "increase_20%_yoy",
		measurement: "daily_revenue_tracking",
		dashboard: "executive_dashboard"
	},
	conversion_rate: {
		target: "3%_or_higher",
		measurement: "orders / sessions",
		dashboard: "operations_dashboard"
	},
	average_order_value: {
		target: "increase_15%_from_baseline",
		measurement: "total_revenue / order_count",
		dashboard: "sales_dashboard"
	},
	customer_acquisition_cost: {
		target: "decrease_10%",
		measurement: "marketing_spend / new_customers",
		dashboard: "marketing_dashboard"
	},
	customer_lifetime_value: {
		target: "increase_25%",
		measurement: "sum_of_customer_purchases_over_life",
		dashboard: "analytics_dashboard"
	},
	repeat_purchase_rate: {
		target: "30%_or_higher",
		measurement: "repeat_customers / total_customers",
		dashboard: "retention_dashboard"
	}
}

TECHNICAL_KPIS={
	system_uptime: {
		target: "99.9%",
		sla: "3_hours_downtime_per_month_maximum",
		measurement: "continuous_health_checks"
	},
	api_response_time: {
		p50_target: "100ms",
		p95_target: "200ms",
		p99_target: "500ms",
		measurement: "application_monitoring"
	},
	error_rate: {
		target: "< 0.5%",
		measurement: "failed_requests / total_requests",
		alert_threshold: "> 5%"
	},
	cache_hit_ratio: {
		target: ">= 70%",
		measurement: "cache_hits / (cache_hits + cache_misses)",
		alert_threshold: "< 50%"
	},
	database_query_performance: {
		p95_target: "100ms",
		p99_target: "500ms",
		measurement: "query_execution_time"
	},
	page_load_time: {
		target: "< 2_seconds",
		measurement: "frontend_performance_metrics",
		monitoring: "google_lighthouse"
	}
}

CUSTOMER_EXPERIENCE_KPIS={
	satisfaction_score: {
		target: ">= 4.5 / 5.0",
		measurement: "post_transaction_survey",
		frequency: "daily_aggregation"
	},
	net_promoter_score: {
		target: ">= 50",
		measurement: "(promoters - detractors) / respondents * 100",
		frequency: "monthly"
	},
	support_ticket_resolution_time: {
		target: "< 24_hours",
		measurement: "time_from_ticket_open_to_resolution",
		sla: "90%_of_tickets_within_sla"
	},
	product_return_rate: {
		target: "< 5%",
		measurement: "returned_items / ordered_items",
		alert_threshold: "> 8%"
	},
	recommendation_ctr: {
		target: ">= 5%",
		measurement: "recommendation_clicks / impressions",
		alert_threshold: "< 3%"
	}
}

</METRICS>

// ==================== END OF ARA CODE ====================


